<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Team 25 - Virtual Therapy</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href='https://fonts.googleapis.com/css?family=Catamaran' rel='stylesheet'>
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.css" rel="stylesheet">
    <!-- Formulas -->
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <!-- <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Virutal therapy</span>
      </a> -->
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav toLeft">
          <div class="mb-5">
          </div>
          <center> <h5> <b>  Virtual Therapy </b><br> </h5> <h6> <b> by Team 25</b></h6></center>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#requirements">Requirements</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#research">Research </a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#hci">HCI</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#design">Design</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#testing">Testing</a>
          </li>
          <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#evaluation">Evaluation</a>
            </li>
            <li class="nav-item">
                <a class="nav-link js-scroll-trigger" href="#management">Management</a>
              </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#References">References</a>
          </li>
        </ul>
      </div>
      <div class="copy" > Copyright © 2017 - All Rights Reserved</div>
    </nav>

    <div class="container-fluid p-0">

      <section class="content-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
            <img align="right" src="img/microsoftlogo.jpg" width="150" height="55">

          <h2 class="mb-0">Virtual

            <span class="text-primary">Therapy</span>


          </h2>

          <div class="subheading mb-3"> "Cognitive Rewiring through mixed-reality" 
                      <!-- <img align="right" src="img/UCL-logo.jpg" width="70" height="70" > -->

          </a>
          </div>

          <p class="toJustify">
              “Virtual Therapy is a 3D audio feedback system created in Unity to help those with visual impairment and 
              lower-back disability. It focuses on improving and correcting the posture of user's through various iterations of pleasant sounds. 
              We hope this can be used globally, and positively affect those who require it”
          </p>          
          



         <!-- <span class="d-none d-lg-block">
            <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
            <b> Rares Dolga:</b>  Team leader, Front End Developer, Back End Developer <b> < email ></b> 
         </span>
  
          <span class="d-none d-lg-block">
              <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
             <b> Faaan Hassan:</b>  Back End Developer, Head of Research <b> < email ></b> 
          </span>
    
          <span class="mb-2 d-none d-lg-block">
                <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
                <b> Cavan Black:</b> Audio, Visual Editor and Report <b>< email ></b>
          </span> -->

           <center> 
    <h3 class="mb-4">Key Features</h3>   
  </center>
    <div class="md-4 row">
       
<!-- <div class="col-sm-1 text-center sr-button"></div> -->
<div class="col-sm-3 text-center sr-button" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6>3D-Sound<br><br>
  </h6><h3><i class="fa fa-headphones fa-3x text-primary" aria-hidden="true"></i></h3>
  <br><br>
    <medium>3D sound. The way natural sound is heard from the left and right ear.
    </medium><br>
    <small></small>
    
</div>
<div class="col-sm-3 text-center sr-button" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6>Pose Correction <br><br>
    </h6><h3><i class="fa fa-male fa-3x text-primary" aria-hidden="true"></i></h3>
     <br><br>
    <medium>Helps the user to correct his/her position through pleasant sound iterations.
      For Example trying to stand straight. 
    </medium><br>
    <small></small>
  
</div>
<div class="col-sm-3 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6> Store Positions<br><br>
    </h6><h3><i class="fa fa-database fa-3x text-primary" aria-hidden="true"></i></h3>
    <br><br>
    <medium> Store the current posture into a database.</medium><br>
    <small></small>
  
</div>

<div class="col-sm-3 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
    <h6> Voice Commands<br><br>
      </h6><h3><i class="fa fa-laptop fa-3x text-primary" aria-hidden="true"></i></h3>
      <br><br>
      <medium> A nice user-interface with voice-activated commands</medium><br>
      <small></small>
    
  </div>
</div>
<br><br>
  <center> 
    <h3 class="mb-3">Features video</h3>   
  </center>
        <center> 
          <iframe class="mb-5" width="700" height="450"
            src="https://www.youtube.com/embed/tgbNymZ7vqY">
          </iframe>
        </center> 

        <center>  
        <h3 class="mb-0">Meet the Team</h3>   
         
           <div class="profiles">
               <img class="mb-0 rounded-circle" src="img/rares.jpg" width="150px">
               <img class="mb-0 rounded-circle" src="img/fazaan.jpg" width="150px">
               <img class="mb-0 rounded-circle" src="img/cavan.jpg" width="150px">
   
           </div>
           
         <div class="description"> 
           <ul class="list-inline">
               <li class="list-inline-item"> <br> <b> Rares Dolga</b> <br> Team leader <br> Front End Developer
                 <br> Back End Developer <br> <b> < rares.dolga.16@ucl.ac.uk ></b> 
               </li>
               <li class="list-inline-item"> <br> <b> Fazaan Hassan</b> <br> Tester<br> Back End Developer <br> 
                 Head of Research <br> <b>< fazaan.hassan.16@ucl.ac.uk></b> 
               </li>
               <li class="list-inline-item"><br> <b> Cavan Black</b> <br> Audio and Visual Lead <br> Researcher <br>  Documentation and Report Editor <br><b>< cavan.black.16@ucl.ac.uk ></b>
               </li>
             </ul>
           </div>
         </center>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="requirements">
        <div class="my-auto">
          <h2 class="mb-3">Requirements</h2>

              <h3 class="mb-1">Project Background </h3>
              <p class="toJustify">
                  The assigned name of the project is “Cognitive rewiring through mixed reality”. The affiliated organisation is Microsoft whom require an application for their user Mark-Pollock. The project is designed to tackle the issue of correcting a user’s posture given the fact that they are visually impaired and paralyzed waist down. Virtual Therapy (Name of the application) is created in such a way, that it can be used alongside the current mechanism of a user correcting his/her position with the help of a psychiatrist. The application is trying to give the user more freedom by relying only on pleasant sound iterations to indicate if they are correctly aligned. The user-interface is developed in mind of further improvements by software engineers in order to provide a friendly interface when using the application, but also for the trainer/psychiatrist to start and stop the program if required so. 
            

              </p>

              <h3 class="mb-1">Client and User</h3>
              <p class="toJustify">
                  The group people part of this project and whom the solution is intended for are as follows: Jarnail Chudge – Client and Project Support, Mark Pollock – Client and Intended user, and finally Dimitry Sayenko – Subject matter expert. Through email communication and skype meetings, we were guided to the right path in terms of the correct technologies and intended hardware to be used for this project. 

              </p>


              <h3 class="mb-1">Requirement Gathering</h3>
              <div class="subheading mb-3">Approach 1 - Structural Interview</div>
              <p>
                  In order to obtain client requirements we had used the method of email communication 
                  to obtain more knowledge of the project. Below we will talk about why chose this and why it was appropaite 
                  for our given situation. 
              </p>
              Why did we choose this?
              <ul>
                <li>Our client was situated in California</li>
                <li>
                    Questions and answers were delivered and responded to quickly
                </li>
                <li>
                  A record was kept of our communication and any misconcepts were handled
                  quickly.
                </li>
                <li>
                    We were able to link (‘Cc’) other people related to the project for example our supervisor Simon.
                </li>
                <li> Below is an example of the questions we asked:</li>
              </ul>

              <div class="mb-3 row">
                  <div class="col-sm-6 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      <!-- <img src="img/interview.jpg" width="85%" height="95%"> -->

                      <ol class="toLeft">
                        <li> <b> What should we aim to develop?</b>
                          <ul>
                              <li> If targeted user's have sight problems, should we use just audio to guide them?</li>
                          </ul>
                          
                        </li>
                        <br>
                        <li> <b>From the research papers, we understood that you want a game that helps disabled people exercise for reconditioning. </b>
                            <ul>
                                <li> How should VR or mixed reality have a role into this?
                                  </li>
                            </ul>
                            
                          </li>
                          <br>

                          <li> <b>
                              From the brief and papers you sent, we understood that the end user is Mark. </b>
                            <ul>
                                <li> Should we build the app just for disabled people who are blind? 

                                  </li>
                                  <li>
                                      Or should the target user's be people with Spinal Cord Injury?
                                  </li>
                            </ul>
                            
                          </li> 
                      </ol>
                      
                    </div>
                    
                    <div class="col-sm-6 text-left sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                        <!-- <img align="left" src="img/rightInterview.jpg" width="100%" height="75%"> -->
                        Reasons for choosing this Initial Method
                        <ul class="toJustify">
                          <li> Due to no prior knowledge the questions had started of basic. This allowed us to gain an in-depth written answer from the client of what is expected</li>
                        <li> The structure of the questions meant we could pinpoint specific areas of the project</li>
                      <li> Allowed us to be prepared for our skype interview as we had gained some knowledge of the client requirements</li>  
                      </ul>
                      What Next?
                      <ul class="toJustify">
                        <li>To be able to create abstract user requirements
                          from the information gathered
                        </li>
                        <li> Have a face to face interview over Skype. This will allow us to
                          have a more informal discussion and understand the porject better
                        </li>
                      </ul>
                      </div>
              </div>


              <div class="subheading mb-3">Approach 2 - Online Questionnaire</div>
              <p class="toJustify">
                  An online Questionnaire was sent to the Mark's physiatrist
                  to retrieve specfic information in order to create the solution that 
                  was required to help him. The information was valuable because as our project
                  would circulate around these requirements. Based on this we could focus on
                  certain joints of the human body opposed to the whole skeleton
                  which would have been beyond the scope of the project.

              </p>
              <center> 
              <img src="img/form1.jpg" width="40%" height="40%">
              <i class="fa fa-arrow-right fa-3x text-primary" aria-hidden="true"></i>
              <img src="img/form2.jpg" width="40%" height="40%">
            </center>

              <div class="subheading mb-3">Approach 3 - Skype Interview</div>
              <p>
                  After gaining an insight of what the 
                  project entails we finally arranged a suitable date to skype interview our client. 
                  Questions were thought of prior to the interview. Upon setting this skype meetings
                  were set every fornite to update our client and user on our progress.
              </p>

              <p> Why did we choose this?</p>
              <ul>
                <li> Build a friendly and trustworthy relationship with the client</li>
                <li> Clear any doubts about current mock-requirements</li>
                <li> Clarify our budget, skills and what is do-able in the given time frame</li>
              </ul>

              

              <h3 class="mb-4">Personas</h3>
              
                <div class="row">
                  <div class="col-sm-6 text-center sr-button" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      <center> <h5>Mark Pollock<br>
                      </h5></center>
                      <img src="img/mark.jpg" width="50%" height="40%">
                      
                      <ul class="list-unstyled">
                        <li> Occupation: Athlete</li>
                        <ii> Role: End User</ii>

                        </ul>
                        <p class="toJustify">
                            When Mark was five, he lost the sight of his right eye and was forced during the remainder of his childhood to avoid contact team sports to preserve the vision in his left eye. He later went on to study Business and Economics in Trinity College, Dublin, where he became the champion of the institution rowers team. At age 22 he lost the sight in his left eye and was then left blind. In 2012, just weeks before his wedding, Pollock fell from an upstairs window, injuring his back and fracturing his skill. This caused internal bleeding on the brain and resulted in long term paralysis. 

                        </p>

                        
                    </div>
                    <div class="md-4 col-sm-6 text-center sr-button" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                        <center> <h5>James Gordon<br>
                        </h5></center>
                        <img src="img/James.jpg" width="50%" height="40%">
                        
                        <ul class="list-unstyled">
                          <li> Occupation: Researcher</li>
                          <ii> Role: Secondary End User</ii>
                          </ul>

                          <p class="toJustify">
                              James Gordon was 8 years old when he had a severe head trauma leaving him blind in both eyes and paralysed waist down. He was educated at the Nottingham High School before graduating and began research into the area of guide dogs and its potential traffic dangers for the blind. He has taken extensive research withint the domain of accessibility for patients that are visually impaired. He has contributed to the "ParaEye" society in UK giving up much of his time to young kids. His work has been renowned by many communities.
                          </p>
                      
                    </div>
                  </div>
              <br><br>
       

              <h3 class="mb-4">Storyboard</h3>
              <center> 
              <img class="mb-4" src="img/storyboardfinal.jpg" width="80%" height="80%">
             </center> 


                <h3 class="mb-4">MoSCoW Requirements</h3>
                    <table class="table table-bordered table-striped" style="font-size:14px">
                        <thead>
                          <tr><th>ID</th><th>Requirement</th><th>Type</th></tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td colspan="3">
                              <b>Must</b>
                            </td>
                          </tr>
                          <tr>
                            <td>
                              R1
                            </td>
                            <td>
                                Gather 3D coordinates of joints from the environment                            </td>
                            <td>
                              Functional
                            </td>
                          </tr>
                          
                          <tr>
                            <td>
                               R2
                            </td>
                            <td>
                                Filter inaccurate data from the sensors to be removed
                            </td>
                            <td>
                               Functional
                            </td>

                          </tr>

                          <tr>
                            <td>
                              R3
                              </td><td>
                                  Reconstruct the skeleton in the Unity environment and map the data received from the sensors
                                </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>  

                          <tr>
                            <td>
                               R4
                            </td>
                            <td>
      
                                Consider the following parts of the body: knee, heel, toe, ankles, hips and mid-back
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>

                          <tr>
                              <td>
                                 R5
                              </td>
                              <td>
        
                                  Develop an algorithm that recognizes wrong alignment of specific joints
        
                              </td>
                              <td>
                                 Non-Functional
                              </td>
  
                            </tr>

                            <tr>
                                <td>
                                   R6
                                </td>
                                <td>
                                    Transform coordinates and angles into 3D sound modulation
          
                                </td>
                                <td>
                                   Functional
                                </td>
    
                              </tr>

                              <tr>
                                  <td>
                                     R7
                                  </td>
                                  <td>
            
                                      Guide User to stand correct in relation to: Trunk / mid-back Extension; Hip Extension; Knee Extension            
                                  </td>
                                  <td>
                                     Functional
                                  </td>
      
                                </tr>

                          <tr>
                            <td colspan="3">
                               <b>Should</b>
                            </td>
                            </tr>
                            <tr>
                                <td>
                                   R1
                                </td>
                                <td>
          
                                    Create Audio User Interface -Main Menu                                </td>
                                </td>
                                <td> 
                                   Functional
                                </td>
    
                              </tr>

                              <tr>
                                  <td>
                                      R2
                                  </td>
                                  <td>
            
                                      Guide User to stand correct in relation to: Trunk / mid-back Extension; Hip Extension; Knee Extension            
                                  </td>
                                  <td>
                                     Functional
                                  </td>
      
                                </tr>

                              <tr> 

                                  <td colspan="3">
                                      <b>Could</b>
                                   </td>
                              </tr>  

                                <tr> 
                            <td>
                               R1
                            </td>
                            <td>
                                Consider other body parts such as shoulders and chest
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>
                          
                          <tr>
                            <td>
                               R2
                            </td>
                            <td>
                                Save multiple posture that are targeted
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>
                      
                          <tr>
                            <td>
                               R3
                            </td>
                            <td>
                                Create a server for connecting to the database
      
                            </td>
                            <td>
                               Functional
                            </td>

                          </tr>

                          <tr>
                              <td>
                                 R4
                              </td>
                              <td>
                                  Create a mongoDB database          
                              </td>
                              <td>
                                 Non-Functional
                              </td>
  
                            </tr>
      
                        </tbody>
                      </table>

              
       
                      <h3 class="mb-1">User Cases</h3>

        <div class="subheading mb-3">Use case Diagram</div>
        <center>
            <img class="mb-4" src="img/usecase.jpg" width="80%" height="80%">
        </center>

        <div class="subheading mb-3">User cases</div>
        <div id="three" class="team-item" style=" text-align:left">
          <center> <b>Use Case 1</b> </center>
          <br>
            <div class="about-cell">
              <table class="table table-bordered table-striped table-responsive">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Calibration of the application
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                               <b> UC1</b> 
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                Ensures headphones are correctly worn
                                and 3D sound can be heard distincly from each ear.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                User
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                <li> System</li>
                                <li> Trainer</li>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. The user runs the application on the machine <br>
                                2. The system displays the main page with voice instructions
                              guiding the user to select an option. 
                            3. Calibration is chosen.</td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <br>
                        <center> <b> Use Case 2</b></center>
                        <br>
                        <table class="table table-bordered table-striped">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Correct User's Posture
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                                <b> UC2 </b>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                The user will move his joints according to sounds in 3D space
                                until he is standing correctly.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                User
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                  <li> System</li>
                                  <li> Trainer</li>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                The kinect sensor must be enabled/working for it
                                to recongize movement. It must also be placed at an
                                appropaite angle and height.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. After the calibration process the user says "Start" as indicated by the instructions <br>
                                2. The screen changes to an avatar and a mapping of the user's joints. This information is for the trainer/psychiatrist.<br>
                                3. A quick rundown of the sounds are played. These indicate which sound is for which joint<br>
                                4. The pose correction algorithm is started and the user is guided to correctly stand.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                Invalid Query: Another option is selected
                              </td>
                            </tr>

                            <tr bgcolor="#42bcf4"><th>
                              <b> ID </b>
                            </th>
                            <td>
                             <b> UC2.1</b>
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Brief Description
                            </th>
                            <td>
                              The user exits the system
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Primary Actors
                            </th>
                            <td>
                              User
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Secondary Actors
                            </th>
                            <td>
                              System
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Preconditions
                            </th>
                            <td>
                              None
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Main Flow
                            </th>
                            <td>
                              1. The user says "Exit" <br>
                              2. Application closes

                            </td>
                          </tr>

                          </tbody>
                        </table><br>

                        <center> <b> Use Case 3</b></center>
                        <br>
                        <table class="table table-bordered table-striped">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Store 3D co-ordinates of the current position
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                                <b>UC3 </b>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                The system captures the current position of the user
                                and stores it into a database. 
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                <li>User </li>
                                <li>Trainer </li>
                                
                                
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                System
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. The user says "Save" <br>
                                2. Rotation, localRotation, position and localPosition are captured for each joint <br>
                                3. A JSON document is created for each "save" and stored in a collection 
                                on a mongoDb server.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                          </tbody>
                        </table><br><br>

              
             
            </div>

        </div>
        
        
        
        
                    
         </div>

      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-3">Research</h2>

          <div class="content-item d-flex flex-column flex-md-row mb-3">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Potential Devices</h3>
              <p class="toJustify">
                  Sensors are a key aspect for our project. We required high quality ‘six degrees of freedom’ devices to retrieve the user’s position in 3D space. This is done in terms of coordinates and relative angles of joints. If the data received is not accurate enough, the overall feedback outputted to the user will have faults. 

              </p>
                  <div class="subheading mb-2">Polhemus Fast-Track</div>
                  <p class="toJustify">
                      It is a promising motion track solution that uses magnetic sources to detect position of smaller devices mounted on the user’s joints. Data received from this sensor is in real time, and their experiments suggest that there is no latency. Also, occlusion does not represent a problem, because the main joints have individual sensors [1]. Occlusion is when one body (including bones), covers another joint, causing obscurity.
                      <br><br>
                      Although this is a good solution in terms of data precision, it is very expensive and exceeds our budged for the project. Furthermore, it requires a considerable amount of time to integrate with an external software so that it can produce 3D sounds.

                  </p>
                  <div class="subheading mb-2">XSENS MV</div>
                  <p class="toJustify">
                      It is a full body motion analysis system capable of giving 3D joint angles, orientation of bones and centre of mass [2]. The first disadvantage is represented by cost - 35,350 Euro is way beyond our budget. Secondly, it has a small delay in sending data by the method of wireless communication and the use of numerous cables. This would not be practical for the user.  Although, compared with Polhenus it has the advantage that it gives the center of mass information 

                  </p>

                  <div class="subheading mb-2">Notch Sensors</div>
                  <p class="toJustify">
                      Our supervisor Simon suggested these sensors as they are cheap and offer reasonable results when gathering joint position. The problem is that the only way they display real time data is by an android application developed by the company. There is no existing API hence making it hard to program as we require a continuous flow of body segments positions to be used in our own piece software for producing 3D sound.  Being composed from 6 sensors, it would be impossible for us to separately track all 20 joints that we need for calculating the pose of a user. Research also suggests that the sensor interferes with metallic surrounding thus obscuring results [3].

                  </p>

                  <div class="subheading mb-2">Kinect V2</div>
                  <p class="toJustify">
                      Kinect is a depth sensor created by Microsoft that outputs the skeleton position of a user in 3D space using the infrared light and RGB camera. It is a cheap and robust solution, with good documentation and online support.<b>  We decided to go with Kinect,</b> because compared with all previous solutions, it offers the possibility of connecting with Unity and because it offers a perfect balance between cost and performance. This part was vital for us because we needed a game engine or another piece software capable of producing mixed reality applications. Also, it provides a SDK that returns the position of up to 25 joints [4]. The device has very performant audio capturing function as well. This is an important aspect for designing a voice-command based user interface for visually impaired people. However, when compared with the previous solutions it does not offer such good results for occlusion. We can to try to reduce this impact by approximating position and inbuilt functions to filter data from interfered joints.
                  </p>

            </div>
          </div>

          <div class="content-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Potential Frameworks</h3>
              <p class="toJustify">
                  The Virtual Therapy project is a mixed reality application, so we required an editor that would facilitate the construction of such a software. This can done by different <b>gaming engines</b>. They are described below:

              </p>
              <div class="subheading mb-2">Unreal</div>
              <p class="toJustify">
                  Unreal is a powerful game engine and has support for VR/AR applications. However, we have not used it because it does not offer support for integrating Kinect. This is a major downside, since Kinect needs wrapper functions to transform the raw data from the sensors.

              </p>

              <div class="subheading mb-2">Unity</div>
              <p class="toJustify">
                  Unity is a game engine that has a great support for virtual reality applications. The biggest advantage is that Kinect has plugins for Unity. This makes it possible to transfer data from the real world to the virtual world. 
                  
                  <b> We have chosen Unity,</b> because it has great built in functions for creating immersive 3D sound feedback but also an accurate depth imager. The choice was also heavily affected by the large amount of documentation available for Unity and the support given on forums. Problems could be solved more quickly as they may have previously been encountered. 

              </p>

            </div>
          </div>

          <div class="content-item d-flex flex-column flex-md-row">
              <div class="resume-content mr-auto">
                <h3 class="mb-2">Potential Programming languages</h3>
                <b> Front End Development</b>
                <p class="toJustify">
                    <ul>
                      <li> Boo </li>
                      <li> JavaScript</li>
                      <li> C#</li>
                    </ul>

                    
                </p>
                <p class="toJustify">
                    Boo language is like python, which it makes easy to learn and use, but there were few documentations for programming Kinect with Boo.
                    The last 2 are the most popular henceforth <b> we have chosen to program in C#. </b>  Reason being is, most of us are familiar with object oriented programming  languages such as Java, which is similar with C#. If we would have written in JavaScript, there would have been issues with script compilation for premade libraries which we will discuss later below. Apart from this, there is no performance advantages of one language over the other.  

                </p>

                 <b> Server Side Development</b>
                <p class="toJustify">
                  <ul>
                    <li> Node JS</li>
                    <li> PHP</li>
                    <li> Ruby</li>
                  </ul>
                </p>
                <p class="toJustify">
                    <b> For the server part we have used Node JS.</b> Alternatives include Django, PHP, Ruby, but node.js has multiple advantages over all.
                    We have decided to use Node JS due to previous experience. This saved time as we did not have to learn a new backend programming language. It also allows the construction of real time web apps, in which both the client and the server can initiate requests. Being an extension of javascript the documentation provided showed that connection of database drivers were can be easily implemented. Also, it can handle a huge number of simultaneous requests which means high scalability. This is important as Microsoft would like to further develop on the application in the future.
                    
                    </p>
                    
                    

                  <b> Potential Databases</b>
                  <br />

                  <br>
                
                <div class="subheading mb-2"> MySQL Relational Database:
                  </div>
                  <p class="toJustify">
                      MySQL is a powerful database, known for high performance and data security. It is one of the best options for complex queries. However, data must follow the same structure. The structure of our data might change, based on the pose chosen by the user. This means the relationship between joints and their number will differ a lot [7]. Also, we have a lot of relationships between the data which will mean introducing many new tables and linking them together in the most efficient manner. Due to this, running queries on this form of data will be expensive. 
                  </p>
                  <div class="subheading mb-2"> MongoDB
                    </div>
                    <p class="toJustify">
                        This database represents an advantage for us because it allows the structure of data to change. Each pose we save is stored as a JSON styled document, making it easier to process. Also, this database scales horizontally, in comparison with first which scales vertically. This means that more traffic is handled by adding more servers. This is more appropriate for out type of data. Considering this we decided to choose Mongo Db [8].
  
                    </p>
              </div>
            </div>

            <div class="content-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <h3 class="mb-2">Libraries and API's</h3>
             
                  


                    <b> Retrieving Joint Positions From The User:
                      </b>
                      <br clear="all" /><br />

                  
                  <div class="subheading mb-2">	OpenNI  NITE
                    </div>
                    <p class="toJustify">
                       The OpenNI is an API used to initialise devices  that contain depth sensors compatible with Prime-sense form application code. We need to start the depth sensor when the unity application is run, get data from it, and stop it on user’s command [6]. For detecting the human body and accurate joint position from Kinect depth images, the NITE library is used.

                    </p>
                  <div class="subheading mb-2">Unity Kinect MS-SDK</div>
                    <p class="toJustify">
                        This is a library that allows us to map data from Kinect to an avatar that represents the human body in a unity world. It contains wrapper classes that make the transformation, using basic mathematical operations like matrix multiplications. As our application needs to initialise and stop the device, it contains drivers necessary for openNI and the kinect sensor. Also, it provides high level functions for using Kinect features. Currently, it does not have any other competitor libraries that uses Kinect for skeleton tracking <b>hence why we are limited to use this one. </b>
                      </p>

                      <div class="subheading mb-2"> OpenPose plus SMPLify
                        </div>
                      <p class="toJustify">
                          A common problem with this software is the lack of integration with Unity. This is required for the augmented part of our project causing problems as no documentation is provided on how to connect the output of library with the game engine we use. This might be considered as an alternative solution in the future of the project, because SMPLify announced that they are working on creating a Unity plugin. Therefore, the only big problem remaining is connecting the avatar to continuous data. However, this cannot be achieved as SMPLify only works on a single image [10]. It is worth mentioning that this library uses high optimisation suggesting that the computation costs for each image is high. Computing a continuous stream of images would not give real time results as the Lag in the application would be high.
                        </p>
                        <p class="toJustify">
                            However, we have not chosen this solution, because the creation of 3D audio feedback involves applying sound on objects in 3D space. Without the third axis, it is impossible to create objects that mimic our user and apply sound towards it from different directions. A solution to this problem would be the use of SMPLify. This is a machine learning software that can create 3D realistic human models from 2D data.  
  
                          </p>

                          <p class="toJustify">
                              A common problem with this software is the lack of integration with gaming engines. This is required for the augmented part of our project causing problems as no documentation is provided on how to connect that avatar with real time streaming of data. This might be considered as an alternative solution in the future of the project, because SMPLify announced that they are working on creating a unity plugin. Therefore, the only big problem remaining is connecting the avatar to continuous data. However, this cannot be achieved as SMPLify only works on a single image [10]. It is worth mentioning that this library uses CNN (convolutional neuronal networks) suggesting that the computation costs for each image is high. Computing a continuous stream of images would not give real time results.
    
                            </p>
                          <b> Speech Recognition</b>
                          <br clear="all" /><br />

                        

                            <div class="subheading mb-2"> Google Speech API
                              </div>
                            <p class="toJustify">
                                This is a known and powerful API that recognizes around 100 languages. It runs on googles cloud services hence why we considered this as a disadvantage. If no internet connection is present, then the application becomes impractical for visual impaired users, as they have no way of interacting with it. 
      
                              </p>

                              <div class="subheading mb-2"> Microsoft Runtime Speech recognition
                                </div>
                              <p class="toJustify">
                                  We needed a service for speech recognition to implement the voice command functionality.  A disadvantage is that the user must install them on the local device to make our app work. <b> We have chosen this SDK</b> because it runs directly in windows 10. It does not require any calls over the internet so the delay in response time is minimal. This is an important factor for a reliable User Interface [11].
        
                                </p>
                              <b> 3D Sound Effect</b>
                              <br clear="all" /><br />

                              
                              <div class="subheading mb-2"> Oculus Rift Spatializer

                                </div>
                              <p class="toJustify">
                                  This plugin can be used in many game engines and has good documentation. Also, it offers the possibility to customize the effects of the sound. It has the disadvantage that it does not imitate the reflexion of sounds from walls very well. In this case the virtual experience we are trying to achieve will not be immersive. 

                                </p>

                                <div class="subheading mb-2"> Microsoft HRTF (Head related transfer function) spatializer

                                  </div>
                                <p class="toJustify">
                                    Unity already contains this plugin, making it easy to use and configure. <b> We have chosen the Microsoft HRTF</b>, because it offers functions to amplify and reduce noise for different room sizes. Sounds can be heard from left and right directions with walls also reflecting noise signals. Because of this the user can easily locate the source of a given sound. This is a clear advantage over the Oculus rift technology making it suitable to achieve our requirements.

                                  </p>
                    </div>

                </div>
              </div>
              
              <div class="content-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-0">algorithms</h3>
                    <p class="toJustify">
                        Pose prediction algorithms are vital for any VR/AR application that include an avatar. Only using the current position of a user to compute new skeleton tracking images for each frame might result in desynchronization [12].
                        To solve this, we consider some well-known mathematical algorithms. 


                    </p>
                    <div class="subheading mb-3">Exponential Filter</div>
                    <p class="toJustify">
                        This is a smoothing filter used for data when a sudden change appears. It applies one exponential function to smooth the input from Kinect. After a period of experimentation, we had observed that this filter does not perfectly follows the data trend in the abstract movement of joints in motion [13].


                    </p>
                    <div class="subheading mb-3">Double Exponential Filter</div>
                    <p class="toJustify">
                        This method predicts various joint positions using a simple linear regression function. We try to estimate the parameters of the equation when studying the movement of the user. The difference between parameters decreases exponentially over time, prioritising the new data from sensor.  <b> This method is preferred because, </b> by applying the exponential function twice the data follows the trend of the real input [12].


                    </p>
                  </div>
                
                </div>

          <div class="content-item d-flex flex-column flex-md-row">
              <div class="resume-content mr-auto">
                <h3 class="mb-1">Final Decision</h3>
                <p class="toJustify">
                    Our final solution consists of a Kinect V2 Sensor to get joint position from the user, and Unity creating the 3D sound effect. Our language of choice was C# for local application and node.js for the backend programming. We chose to use MS-DK Kinect from the Unity Asset Store with OpenNi to control the Kinect sensor and retrieve data from it. This library also contained wrappers for transforming input into “unity spatial coordinates” allowing us to map to an avatar. We also decided to use the double Exponential Filter to predict the continuous flow data and reduce the phenomenon of occlusion as much as possible.  We also decided to use the Microsoft HRTF spatializer to create the 3D sound effect. 

                </p>
              </div>
            </div>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="hci">
        <div class="my-auto">
          <h2 class="mb-3">HCI</h2>

          <h3 class="mb-2">Sketches</h3>
          <p class="toJustify">
              As part of the HCI requirement we created sketches to visualise how our solution would work.
              However, as the app does provide visual interaction for the user, we have made an overview of the components that it may consist of. 
              The sketches represent how (before realising that we would require a different approach) the sensors will interact with movement.
              Below are the 2 main sketeches.   
          </p>
 
          <center>
              <img class="mb-3" src="img/sketch.jpg" width="85%" height="85%">
          </center>


          <br>
          <h3 class="mb-3">Wireframes</h3>
          <p class="toJustify">
              The skecthes then allowed us create a wireframe. It is important to note that this is not the way the solution is implemented, but is the visualisation of the potential way of solving and creating the solution.    
            The Final desgin was crated after conversing with the client and project support. The use of the hololens was appropriate as it eliminated the need of wires.      </p>
          <center>
              <img src="img/wireframefinal2.jpg" width="85%" height="85%">
          </center>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="design">
        <div class="my-auto">
          <h2 class="mb-3">Design</h2>

          <h3 class="mb-3">System Architecture Diagram</h3>
          <center> 
          <img src="img/hiarchfinal.jpg" width="85%" height="80%">
          </center>

          <b> Component A</b>
          <p class="toJustify">
              This part of the system architecture diagram shows how the user interacts with the overall system. The headphones indicate the usage of 3D as well as granting the immersive expierence. The user here is intended to be Mark Pollock.

          </p>
          <b> Component B</b>
          <p class="toJustify">
              This represents the Kinect 2 Sensor which retrieves data from componenet A (the user), using its depth sensor. It captures continuous frames of movement. It must be noted that this piece of hardware must be placed at a suitable height and a distance from the user to fully capture the skeleton image. 

          </p>
          <b> Component C</b>
          <p class="toJustify">
              The database Icon represents the applications memory when running the scripts. It processes the data from the Kinect sensor and identifies whether the correct position has been achieved. It has also has a series of sound scripts stored which are attached to the certain joints (game objects of the avatar).

          </p>
          <b> Component D</b>
          <p class="toJustify">
              Component E is shows the visual interface of the unity application. It works with the Kinect Sensor (component B) to display the movement of the user as a avatar. 

          </p>
          <b> Component E</b>
          <p class="toJustify">
              The soundscripts were created externally and then imported into unity. These are natural-monotone sounds which increase or decrease in relation to the movement of the user. The scripts are attatched to specific joints as mentioned before and are played when a certain parts of the body move - For example the extension of the knee when trying to stand straight. The sounds are transmitted to the user via headphones in which the person then corrects his own position.
            </p>
          <h3 class="mb-3">Class Diagram</h3>
            <center>
                <img src="img/ClassDiagram.jpg" width="90%" height="90%">
            </center>
          <h3 class="mb-3">Design Patterns used</h3>
          <h3 class="mb-3">Data Storage</h3>
          <h3 class="mb-3">Key functionalities </h3>
          <b> Gather 3D joints from the environment and map them to an Avatar
            </b>
            <p class="toJustify">
                In order to track the movement of the user’s body in real time we used the Kinect device and the Kinect SDK created by Microsoft. It must be mentioned that the tracking algorithm implemented in the sensor works with face detection, which means that the patient must face the Kinect to have his limbs position monitored. Once data is received from the device, we must process it and transform from the raw format into Unity spatial coordinates. This transformation is done using wrapper classes from the MS-SDK library we imported. 
                <br>
                Furthermore, we filtered the converted data using the double exponential filter method, described in the research part. The process of filtering is necessary because occlusion (misalignment of joints) can occur, meaning that Kinect gives inaccurate results when compared with real coordinates. Also, an approximation might be needed when some joint changes its position between different frames. If no approximation is done, a sudden difference in avatar’s position will be noticeable, making it desynchronized from the user’s movements.
              <br>
              After correctly configuring the above steps, we must map the retrieved information to game objects which are represented as bones of the avatar. All virtual bones are arranged into a hierarchy which denotes the human skeleton.

            </p>
            <b> Develop an algorithm that detects incorrect pose of the user
              </b>
            <p class="toJustify">
                This requirement had been quite challenging due to the numerous ways of tackling this problem. We required a unique which applied to a simplified version of our task. There are many machine learning algorithms that can indicate and perceive the pose of the user, but are computationally expensive. This factor is an obstacle for giving real time feedback. To avoid this problem, we designed a rule-based system, that gives feedback on the pose by considering the relation between multiple joints. For example, to determine how bent the left knee is, we calculate the angle between 3 points: left hip, ankle and knee. Those joints form 2 vectors in 3d space, which makes it easy to calculate the angle using this formula:
                <br>
               <center> <math xmlns="http://www.w3.org/1998/Math/MathML"> 
                <mi>𝜃
                    </mi><mo>=</mo><msup><mi>cos</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup>  <mfrac> <mi> |P|  x  |Q|</mi><mn>P x Q</mn> </mfrac></mfrac></math>  <i> where P, Q are the vectors defined by the joint points</i></center> 
               
                <br>
                We know that the knee has a correct position when the value of the angle is inside an interval of error. 
                As the definition of correct joint alignment depends on the user’s preferences, we define the conditions as variables which are calculated from the predefined choice of avatar model. In our case the clients clearly defined what a correct pose means. Below is an example of what a imbalanced and balanced skeleton looks like.
                <center>
                    <img src="img/keyfunc.jpg" width="60%" height="60%">

                </center>
                Different body parts can affect human position independently of the other limbs, which is the case of spine base. The solution is to simply take their rotation in space.   Despite there are distinct conditions, our system can handle all different approaches, because there are just a limited number of possibilities to consider. 

              </p>

                <b> Transform feedback in 3D sound modulation
                </b>
                <p class="toJustify">
                    As our end user was visual impaired, he required audio guidance to know how to correctly align his joints. Simple verbal instructions were not enough, because they could not reflect how the user was progressing in terms of his current state. In other words, the verbal instructions did not suggest when knees are bent at an angle of 5 degrees and when at 60 degrees. There was not differentiation between the two. Instead, we took the approach of trying to localise the joints with 3D sound. In real life, we can distinguish sources of noise by how far they are and from which direction.  To avoid an unnecessary large learning curve, we can allocate keywords to each joint. This means we can combine the 3D sound affect principal along with this to create a tailored experience for the user.
                    To accomplish an immersive experience, we used the Microsoft’s HRTF spatializer (Discussed in the research part). 
                    <br>
                    <br>
                    As the role of avatar has been clearly defined, we need to create the 3D sound effects that represent the source of noise to be present in the scene. Taking the head of the humanoid (The avatar) as listener, position of those objects can be easily deduced from sound modulation.  The patient will have the feeling of being substituted in place of the avatar in the virtual world hence being be able to realise where his joints are localised in space.
                    <br>
                    <br>
                    Despite the solution being good, we faced numerous challenges.  Firstly, the spatializer was built to imitate reality as much as possible. This meant that large distances have a noticeable effect on the sound volume. In our case, even 5 degrees of motion were extremely important and needed to be signalled to be changed. To solve this, we created mock joints. In the Unity scene, these move exponentially in relation to the user’s joints on a specific axis. For example, if the left hip is not properly aligned with the rest of the body (pushed too far on the left) than there is a difference <b> <i> Dx</i></b> between the correct location and the current location on the x axis. Considering that value of <b> <i> Dx</i></b> is constrained by the human body shape and is rigid (i.e it belongs to [-1, 1]) we can compute a new position for a mock hip by applying the function <b><i> Exp(Dx) = 10*Dx</i></b>. The newly created point will have the same values on y and z axis as the human joint while the x value will be given the previously mentioned function. The new object will then have a sound source attached to it. The schema below explains the concept:
                   <center>
                      <img src="img/keyfuncscheme.jpg" width="60%" height="60%">
                   </center> 
                   Secondly, a wrong pose can be the result of multiple joints that need correction. In this case, we cannot play at once all the various noises mapped to those joints. This will cause large amount of confusion for the user. Instead we try to choose the correct joint that has the maximal error. However, different limbs have different acceptance intervals of distinct ranges. We solved the issue by considering the proportions of joints and limbs and calculating the percentage error obtained from each proportion. 

                </p>
                <b>Main Menu with Voice activated Commands
                  </b>
                  <p class="toJustify">All users should be able to interact with the application in a friendly and easy-to-use manner. Since our target audience are visually impaired people we had agreed with the client to construct a menu based on speech recognition. Our solution utilises a small grammar of several instructions combined with the speech recognition features of Windows 10. Furthermore, Kinect has a very performant microphone that allowed us to create a reliable user interface. The only problem with the speech recognition system is its vulnerability to external sounds. This is the case for all speech engines.
                    </p>
        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="testing">
        <div class="my-auto">
          <h2 class="mb-3">Testing</h2>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="evaluation">
          <div class="my-auto">
            <h2 class="mb-3">Evaluation</h2>

            <div class="content-item d-flex flex-column flex-md-row mb-3">
                <div class="resume-content mr-auto">
                  <h3 class="mb-0">Achievement Table</h3>
                  <center>
                      <img src="img/achievement.jpg" width="80%" height="90%">

                  </center>
                </div>
              </div>

              <div class="content-item d-flex flex-column flex-md-row mb-3">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-0">Bug Table</h3>
  
                  </div>
                </div>

                <div class="content-item d-flex flex-column flex-md-row mb-3">
                    <div class="resume-content mr-auto">
                      <h3 class="mb-0">Contribution Table</h3>
                      <center>
                          <img src="img/contribution.jpg" width="80%" height="90%">

                      </center>
                    </div>
                  </div>

                  <div class="content-item d-flex flex-column flex-md-row mb-3">
                      <div class="resume-content mr-auto">
                        <h3 class="mb-0">Evaluation</h3>
      
                      </div>
                    </div>
    
                    <div class="content-item d-flex flex-column flex-md-row mb">
                        <div class="resume-content mr-auto">
                          <h3 class="mb-0">Future Work</h3>
        
                        </div>
                      </div>

          </div>
        </section>

        <section class="content-section p-3 p-lg-5 d-flex flex-column" id="management">
            <div class="my-auto">
              <h2 class="mb-5">Management</h2>

              <div class="content-item d-flex flex-column flex-md-row mb-3">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-0">Deployment Manual</h3>
  
                  </div>
                </div>

                <div class="content-item d-flex flex-column flex-md-row mb-3">
                    <div class="resume-content mr-auto">
                      <h3 class="mb-0">Gantt Chart</h3>
                    

                     <center>

      
                      <img src="img/ganttchart2.jpg" width="95%" height="90%">
                    </center>
                    </div>
                  </div>

            </div>
          </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="References">
        <div class="my-auto">
          <h2 class="mb-3">References</h2>
          <ul class="list-unstyled">
            <li>
                [1] Polhemus Fast-Track: Available at: <a href="https://polhemus.com/motion-tracking/all-trackers/fastrak#collapseOne">https://polhemus.com/</a>

            </li>
            <li>[2] XSENS: Available at: <a href="https://www.xsens.com/tags/sports-biomechanics/">https://www.xsens.com/tags/sports-biomechanics/</a>
              </li>
              <li>[3] NOTCH: Available at: <a href="https://wearnotch.com/">https://wearnotch.com/</a>
                </li>
                <li> [4] Kinect: Available at: <a href="https://msdn.microsoft.com/en-us/library/dn758675.aspx">https://msdn.microsoft.com/en-us/library/dn758675.aspx</a>
                  </li>
                  <li> [5] Paper Kinect: Available at: <a href="https://www.cla.purdue.edu/academic/vpa/ad/act/resources/ad41700_barrett_kinect_unity.pdf">https://www.cla.purdue.edu/academic/vpa/ad/act/resources/ad41700_barrett_kinect_unity.pdf</a>
                    </li>
                    <li>[6] OpenNi: Available at: <a href="https://s3.amazonaws.com/com.occipital.openni/OpenNI_Programmers_Guide.pdf">https://s3.amazonaws.com/com.occipital.openni/OpenNI_Programmers_Guide.pdf</a>
                      </li>
                      <li>[7] MySQL: Available at: <a href="https://dev.mysql.com/doc/refman/8.0/en/">https://dev.mysql.com/doc/refman/8.0/en/</a>
                        </li>
                        <li>
                            [8] Mongo Db: Available at: <a href="https://docs.mongodb.com/manual/">https://docs.mongodb.com/manual/</a>

                        </li>
                        <li> [9] OpenPose: Available at: <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/README.md">https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/README.md </a>
                          </li>
                          <li> [10] SMPLify: Available at: <a href=" http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf"> http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf</a>
                            </li>
                            <li> [11] Speech Microsoft: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx"> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                              </li>
                              <li>[12] Joint Filters: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx"> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                                </li>
                                <li> [13] Double Exponential Microsoft: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx "> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                                  </li>
          </ul>


          <!-- <h3 class="mb-2">Contact Details</h>
          <h3 class="mb-0"> Location </h3>
          <p>Gower Street
          <br>London, WC1E 6HH</p>

          <h3 class="mb-0">The Team</h2>
          <p>Rares Dolga 
             | Fazaan Hassan |
              Calvan Black</p> -->
         <center> <p style = "margin-top: -10px">&Copy; All rights reserved</p></center> 


        </div>
      </section>

    </div>


    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>


  </body>

</html>
