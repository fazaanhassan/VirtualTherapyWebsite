<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Team 25 - VirtualTherapy</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href='https://fonts.googleapis.com/css?family=Catamaran' rel='stylesheet'>
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.css" rel="stylesheet">
    <!-- Formulas -->
    <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <!-- <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Virutal therapy</span>
      </a> -->
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav toLeft">
          <div class="mb-5">
          </div>
          <center> <h5> <b>  VirtualTherapy </b><br> </h5> <h6> <b> by Team 25</b></h6></center>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">Home</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#requirements">Requirements</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#research">Research </a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#hci">HCI</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#design">Design</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#testing">Testing</a>
          </li>
          <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#evaluation">Evaluation</a>
            </li>
            <li class="nav-item">
                <a class="nav-link js-scroll-trigger" href="#management">Management</a>
              </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#References">References</a>
          </li>
        </ul>
      </div>
      <div class="copy" > Copyright © 2018 - All Rights Reserved</div>
    </nav>

    <div class="container-fluid p-0">

      <section class="content-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
            <img align="right" src="img/microsoftlogo.jpg" width="150" height="55">
            <img align="right" src="img/UCL-logo.jpg" width="70" height="70" >

          <h2 class="mb-0">Virtual

            <span class="text-primary">Therapy</span>


          </h2>

          <div class="subheading mb-3"> "Cognitive Rewiring through mixed-reality" 
            <br clear="all" /><br />

          </a>
          </div>

          <p class="toJustify">
              “Virtual Therapy is a 3D audio feedback system created in Unity to help those with visual impairment and lower-back disability. It focuses on improving and correcting the posture of user's through various iterations of pleasant sounds.
              <br><br>
              The problem is that there is a need for automatic computer feedback in therapy sessions that would make the user understand the alignment of his joints. Our approach uses sensors to gather users joint position and rule-based algorithms for pose detection. 3D sound was used as a form of intuitive feedback on how to correct the pose.
              <br><br>
              We successfully developed a system that tackles this open research problem using innovative technologies. We hope this can be used globally, and positively affect those who require it. 
              ”
          </p>          
          <br clear="all" /><br />




         <!-- <span class="d-none d-lg-block">
            <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
            <b> Rares Dolga:</b>  Team leader, Front End Developer, Back End Developer <b> < email ></b> 
         </span>
  
          <span class="d-none d-lg-block">
              <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
             <b> Faaan Hassan:</b>  Back End Developer, Head of Research <b> < email ></b> 
          </span>
    
          <span class="mb-2 d-none d-lg-block">
                <img class="mb-3 rounded-circle" src="img/profile.jpg" width="100px">
                <b> Cavan Black:</b> Audio, Visual Editor and Report <b>< email ></b>
          </span> -->

           <center> 
    <h3 class="mb-4">Key Features</h3>   
  </center>
    <div class="md-4 row">
       
<!-- <div class="col-sm-1 text-center sr-button"></div> -->
<div class="col-sm-3 text-center sr-button" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6>3D-Sound<br><br>
  </h6><h3><i class="fa fa-headphones fa-3x text-primary" aria-hidden="true"></i></h3>
  <br><br>
    <medium>3D sound. The way natural sound is heard from the left and right ear.
    </medium><br>
    <small></small>
    
</div>
<div class="col-sm-3 text-center sr-button" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6>Pose Correction <br><br>
    </h6><h3><i class="fa fa-male fa-3x text-primary" aria-hidden="true"></i></h3>
     <br><br>
    <medium>Helps the user to correct his/her posture through pleasant sound iterations.
      For Example trying to stand straight. 
    </medium><br>
    <small></small>
  
</div>
<div class="col-sm-3 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
  <h6> Store Positions<br><br>
    </h6><h3><i class="fa fa-database fa-3x text-primary" aria-hidden="true"></i></h3>
    <br><br>
    <medium> Store the current posture into a database.</medium><br>
    <small></small>
  
</div>

<div class="col-sm-3 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
    <h6> Voice Commands<br><br>
      </h6><h3><i class="fa fa-laptop fa-3x text-primary" aria-hidden="true"></i></h3>
      <br><br>
      <medium> A nice user-interface with voice-activated commands</medium><br>
      <small></small>
    
  </div>
</div>

<br><br>
  <center> 
    <h3 class="mb-3">Features video</h3>   
  </center>

              <center>

                <iframe width="560" height="315" src="https://www.youtube.com/embed/gs0b5Cnryec" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>            </center>
        </center> 
          <br>
        <center>  
        <h3 class="mb-0">Meet the Team</h3>   
         
           <div class="profiles">
               <img class="mb-0 rounded-circle" src="img/rares.jpg" width="150px">
               <img class="mb-0 rounded-circle" src="img/fazaan.jpg" width="150px">
               <img class="mb-0 rounded-circle" src="img/cavan.jpg" width="150px">
   
           </div>
           
         <div class="description"> 
           <ul class="list-inline">
               <li class="list-inline-item"> <br> <b> Rares Dolga</b> <br> Team leader <br> Front End Developer
                 <br> Back End Developer <br> <b> < rares.dolga.16@ucl.ac.uk ></b> 
               </li>
               <li class="list-inline-item"> <br> <b> Fazaan Hassan</b> <br> Tester<br> Back End Developer <br> 
                 Head of Research <br> <b>< fazaan.hassan.16@ucl.ac.uk></b> 
               </li>
               <li class="list-inline-item"><br> <b> Cavan Black</b> <br> Audio and Visual Lead <br> Researcher <br>  Documentation and Report Editor <br><b>< cavan.black.16@ucl.ac.uk ></b>
               </li>
             </ul>
           </div>
         </center>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="requirements">
        <div class="my-auto">
          <h2 class="mb-3">Requirements</h2>

              <h3 class="mb-1">Project Background </h3>
              <p class="toJustify">
                  The assigned name of the project is “Cognitive rewiring through mixed reality”. The affiliated organisation is Microsoft whom require an application for their user Mark-Pollock. VirtualTherapy (Name of the application) is created in such a way, that it can be used alongside the current mechanism of a user correcting his/her position with the help of a psychiatrist. The user-interface is developed in mind of further improvements by software engineers in order to provide a friendly interface when using the application, but also for the trainer/psychiatrist to start and stop the program if required so. 
            

              </p>

              <h3 class="mb-1">Client and User</h3>
              <p class="toJustify">
                  The group people part of this project and whom the solution is intended for are as follows: Jarnail Chudge – Client and Project Support, Mark Pollock – Client and Intended user, and finally Dimitry Sayenko – Subject matter expert. Through email communication and skype meetings, we were guided to the right path in terms of the correct technologies and intended hardware to be used for this project. 

              </p>

              <h3 class="mb-1">Project Goals</h3>
              <p class="toJustify">
                The project is designed to tackle the issue of correcting a user’s posture given the fact that they are visually impaired and paralyzed waist down.
                The application is trying to give the user more freedom by relying only on pleasant sound iterations to indicate if they are correctly aligned.
            </p>


              <h3 class="mb-1">Requirement Gathering</h3>
              <div class="subheading mb-3">Approach 1 - Structural Interview</div>
              <p>
                  In order to obtain client requirements we had used the method of email communication 
                  to obtain more knowledge of the project. Below we will talk about why chose this and why it was appropriate 
                  for our given situation. 
              </p>
              Why did we choose this?
              <ul>
                <li>Our client was situated in California</li>
                <li>
                    Questions and answers were delivered and responded to quickly
                </li>
                <li>
                  A record was kept of our communication and any misconceptions were handled
                  quickly.
                </li>
                <li>
                    We were able to link (‘Cc’) other people related to the project for example our supervisor Simon.
                </li>
                <li> Below is an example of the questions we asked:</li>
              </ul>

              <div class="mb-3 row">
                  <div class="col-sm-6 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      <!-- <img src="img/interview.jpg" width="85%" height="95%"> -->

                      <ol class="toLeft">
                        <li> <b> What should we aim to develop?</b>
                          <ul>
                              <li> If targeted user's have sight problems, should we use just audio to guide them?</li>
                          </ul>
                          
                        </li>
                        <br>
                        <li> <b>From the research papers, we understood that you want a game that helps disabled people exercise for reconditioning. </b>
                            <ul>
                                <li> How should VR or mixed reality have a role into this?
                                  </li>
                            </ul>
                            
                          </li>
                          <br>

                          <li> <b>
                              From the brief and papers you sent, we understood that the end user is Mark. </b>
                            <ul>
                                <li> Should we build the app just for disabled people who are blind? 

                                  </li>
                                  <li>
                                      Or should the target user's be people with Spinal Cord Injury?
                                  </li>
                            </ul>
                            
                          </li> 
                      </ol>
                      
                    </div>
                    
                    <div class="col-sm-6 text-left sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                        <!-- <img align="left" src="img/rightInterview.jpg" width="100%" height="75%"> -->
                        Reasons for choosing this Initial Method
                        <ul class="toJustify">
                          <li> Due to no prior knowledge the questions had started of basic. This allowed us to gain an in-depth written answer from the client of what is expected</li>
                        <li> The structure of the questions meant we could pinpoint specific areas of the project</li>
                      <li> Allowed us to be prepared for our skype interview as we had gained some knowledge of the client requirements</li>  
                      </ul>
                      What Next?
                      <ul class="toJustify">
                        <li>To be able to create abstract user requirements
                          from the information gathered
                        </li>
                        <li> Have a face to face interview over Skype. This will allow us to
                          have a more informal discussion and understand the project better
                        </li>
                      </ul>
                      </div>
              </div>


              <div class="subheading mb-3">Approach 2 - Online Questionnaire</div>
              <p class="toJustify">
                  An online Questionnaire was sent to the Mark's physiatrist
                  to retrieve specific information in order to create the solution that 
                  was required to help him. The information was valuable because as our project
                  would circulate around these requirements. Based on this we could focus on
                  certain joints of the human body opposed to the whole skeleton
                  which would have been beyond the scope of the project.

              </p>
              <center> 
              <img src="img/form1.jpg" width="40%" height="40%">
              <i class="fa fa-arrow-right fa-3x text-primary" aria-hidden="true"></i>
              <img src="img/form2.jpg" width="40%" height="40%">
            </center>

              <div class="subheading mb-3">Approach 3 - Skype Interview</div>
              <p>
                  After gaining an insight of what the 
                  project entails we finally arranged a suitable date to skype interview our client. 
                  Questions were thought of prior to the interview. Upon setting this skype meetings
                  were set every fornite to update our client and user on our progress.
              </p>

              <p> Why did we choose this?</p>
              <ul>
                <li> Build a friendly and trustworthy relationship with the client</li>
                <li> Clear any doubts about current mock-requirements</li>
                <li> Clarify our budget, skills and what is doable in the given time frame</li>
              </ul>

              

              <h3 class="mb-4">Personas</h3>
              
                <div class="row">
                  <div class="col-sm-6 text-center sr-button" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      <center> <h5>Mark Pollock<br>
                      </h5></center>
                      <img src="img/mark.jpg" width="50%" height="40%">
                      
                      <ul class="list-unstyled">
                        <li> Occupation: Athlete</li>
                        <ii> Role: End User</ii>

                        </ul>
                        <p class="toJustify">
                            When Mark was five, he lost the sight of his right eye and was forced during the remainder of his childhood to avoid contact team sports to preserve the vision in his left eye. He later went on to study Business and Economics in Trinity College, Dublin, where he became the champion of the institution rowers team. At age 22 he lost the sight in his left eye and was then left blind. In 2012, just weeks before his wedding, Pollock fell from an upstairs window, injuring his back and fracturing his skill. This caused internal bleeding on the brain and resulted in long term paralysis. 

                        </p>

                        
                    </div>
                    <div class="md-4 col-sm-6 text-center sr-button" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                        <center> <h5>James Gordon<br>
                        </h5></center>
                        <img src="img/James.jpg" width="50%" height="40%">
                        
                        <ul class="list-unstyled">
                          <li> Occupation: Researcher</li>
                          <ii> Role: Secondary End User</ii>
                          </ul>

                          <p class="toJustify">
                              James Gordon was 8 years old when he had a severe head trauma leaving him blind in both eyes and paralysed waist down. He was educated at the Nottingham High School before graduating and began research into the area of guide dogs and its potential traffic dangers for the blind. He has taken extensive research within the domain of accessibility for patients that are visually impaired. He has contributed to the "ParaEye" society in UK giving up much of his time to young kids. His work has been renowned by many communities.
                          </p>
                      
                    </div>
                  </div>
              <br><br>
       

              <h3 class="mb-4">Storyboard</h3>
              <center> 
              <img class="mb-4" src="img/storyboardfinal.jpg" width="80%" height="80%">
             </center> 


                <h3 class="mb-4">MoSCoW Requirements</h3>
                    <table class="table table-bordered table-striped" style="font-size:14px">
                        <thead>
                          <tr><th>ID</th><th>Requirement</th><th>Type</th></tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td colspan="3">
                              <b>Must</b>
                            </td>
                          </tr>
                          <tr>
                            <td>
                              R1
                            </td>
                            <td>
                                Gather 3D coordinates of joints from the environment                            </td>
                            <td>
                              Functional
                            </td>
                          </tr>
                          
                          <tr>
                            <td>
                               R2
                            </td>
                            <td>
                                Filter inaccurate data from the sensors to be removed
                            </td>
                            <td>
                               Functional
                            </td>

                          </tr>

                          <tr>
                            <td>
                              R3
                              </td><td>
                                  Reconstruct the skeleton in the Unity environment and map the data received from the sensors
                                </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>  

                          <tr>
                            <td>
                               R4
                            </td>
                            <td>
      
                                Consider the following parts of the body: knee, heel, toe, ankles, hips and mid-back
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>

                          <tr>
                              <td>
                                 R5
                              </td>
                              <td>
        
                                  Develop an algorithm that recognizes wrong alignment of specific joints
        
                              </td>
                              <td>
                                 Non-Functional
                              </td>
  
                            </tr>

                            <tr>
                                <td>
                                   R6
                                </td>
                                <td>
                                    Transform coordinates and angles into 3D sound modulation
          
                                </td>
                                <td>
                                   Functional
                                </td>
    
                              </tr>

                              <tr>
                                  <td>
                                     R7
                                  </td>
                                  <td>
            
                                      Guide User to stand correct in relation to: Trunk / mid-back Extension; Hip Extension; Knee Extension            
                                  </td>
                                  <td>
                                     Functional
                                  </td>
      
                                </tr>

                          <tr>
                            <td colspan="3">
                               <b>Should</b>
                            </td>
                            </tr>
                            <tr>
                                <td>
                                   R1
                                </td>
                                <td>
          
                                    Create Audio User Interface -Main Menu                                </td>
                                </td>
                                <td> 
                                   Functional
                                </td>
    
                              </tr>

                              <tr>
                                  <td>
                                      R2
                                  </td>
                                  <td>
            
                                      Guide User to stand correct in relation to: Trunk / mid-back Extension; Hip Extension; Knee Extension            
                                  </td>
                                  <td>
                                     Functional
                                  </td>
      
                                </tr>

                              <tr> 

                                  <td colspan="3">
                                      <b>Could</b>
                                   </td>
                              </tr>  

                                <tr> 
                            <td>
                               R1
                            </td>
                            <td>
                                Consider other body parts such as shoulders and chest
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>
                          
                          <tr>
                            <td>
                               R2
                            </td>
                            <td>
                                Save multiple posture that are targeted
      
                            </td>
                            <td>
                               Non-Functional
                            </td>

                          </tr>
                      
                          <tr>
                            <td>
                               R3
                            </td>
                            <td>
                                Create a server for connecting to the database
      
                            </td>
                            <td>
                               Functional
                            </td>

                          </tr>

                          <tr>
                              <td>
                                 R4
                              </td>
                              <td>
                                  Create a mongoDB database          
                              </td>
                              <td>
                                 Non-Functional
                              </td>
  
                            </tr>
      
                        </tbody>
                      </table>

              
       
                      <h3 class="mb-1">User Cases</h3>

        <div class="subheading mb-3">Use case Diagram</div>
        <center>
            <img class="mb-4" src="img/usecase.jpg" width="80%" height="80%">
        </center>

        <div class="subheading mb-3">User cases</div>
        <div id="three" class="team-item" style=" text-align:left">
          <center> <b>Use Case 1</b> </center>
          <br>
            <div class="about-cell">
              <table class="table table-bordered table-striped table-responsive">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Calibration of the application
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                               <b> UC1</b> 
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                Ensures headphones are correctly worn
                                and 3D sound can be heard distincly from each ear.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                User
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                <li> System</li>
                                <li> Trainer</li>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. The user runs the application on the machine <br>
                                2. The system displays the main page with voice instructions
                              guiding the user to select an option. 
                            3. Calibration is chosen.</td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <br>
                        <center> <b> Use Case 2</b></center>
                        <br>
                        <table class="table table-bordered table-striped">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Correct User's Posture
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                                <b> UC2 </b>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                The user will move his joints according to sounds in 3D space
                                until he is standing correctly.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                User
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                  <li> System</li>
                                  <li> Trainer</li>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                The kinect sensor must be enabled/working for it
                                to recongize movement. It must also be placed at an
                                appropaite angle and height.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. After the calibration process the user says "Start" as indicated by the instructions <br>
                                2. The screen changes to an avatar and a mapping of the user's joints. This information is for the trainer/psychiatrist.<br>
                                3. A quick rundown of the sounds are played. These indicate which sound is for which joint<br>
                                4. The pose correction algorithm is started and the user is guided to correctly stand.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                Invalid Query: Another option is selected
                              </td>
                            </tr>

                            <tr bgcolor="#42bcf4"><th>
                              <b> ID </b>
                            </th>
                            <td>
                             <b> UC2.1</b>
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Brief Description
                            </th>
                            <td>
                              The user exits the system
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Primary Actors
                            </th>
                            <td>
                              User
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Secondary Actors
                            </th>
                            <td>
                              System
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Preconditions
                            </th>
                            <td>
                              None
                            </td>
                          </tr>
                          <tr>
                            <th>
                              Main Flow
                            </th>
                            <td>
                              1. The user says "Exit" <br>
                              2. Application closes

                            </td>
                          </tr>

                          </tbody>
                        </table><br>

                        <center> <b> Use Case 3</b></center>
                        <br>
                        <table class="table table-bordered table-striped">
                          <tbody>
                            <tr>
                              <th>
                                Use Case
                              </th>
                              <td>
                                Store 3D co-ordinates of the current position
                              </td>
                            </tr>
                            <tr bgcolor="#42bcf4">
                              <th>
                                <b> ID</b>
                              </th>
                              <td>
                                <b>UC3 </b>
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Brief Description
                              </th>
                              <td>
                                The system captures the current position of the user
                                and stores it into a database. 
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Primary Actors
                              </th>
                              <td>
                                <li>User </li>
                                <li>Trainer </li>
                                
                                
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Secondary Actors
                              </th>
                              <td>
                                System
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Preconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Main Flow
                              </th>
                              <td>
                                1. The user says "Save" <br>
                                2. Rotation, localRotation, position and localPosition are captured for each joint <br>
                                3. A JSON document is created for each "save" and stored in a collection 
                                on a mongoDb server.
                              </td>
                            </tr>
                            <tr>
                              <th>
                                Postconditions
                              </th>
                              <td>
                                None
                              </td>
                            </tr>

                            <tr>
                              <th>
                                Alternative Flows
                              </th>
                              <td>
                                None
                              </td>
                            </tr>
                          </tbody>
                        </table><br><br>

              
             
            </div>

        </div>
        
        
        
        
                    
         </div>

      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-3">Research</h2>

          <div class="content-item d-flex flex-column flex-md-row mb-3">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Potential Devices</h3>
              <p class="toJustify">
                  Sensors are a key aspect for our project. We required high quality ‘six degrees of freedom’ devices to retrieve the user’s position in 3D space. This is done in terms of coordinates and relative angles of joints. If the data received is not accurate enough, the overall feedback outputted to the user will have faults. 

              </p>
                  <div class="subheading mb-2">Polhemus Fast-Track</div>
                  <p class="toJustify">
                      It is a promising motion track solution that uses magnetic sources to detect position of smaller devices mounted on the user’s joints. Data received from this sensor is in real time, and their experiments suggest that there is no latency. Also, occlusion does not represent a problem, because the main joints have individual sensors [1]. Occlusion is when one body (including bones), covers another joint, causing obscurity.
                      <br><br>
                      Although this is a good solution in terms of data precision, it is very expensive and exceeds our budged for the project. Furthermore, it requires a considerable amount of time to integrate with an external software so that it can produce 3D sounds.

                  </p>
                  <div class="subheading mb-2">XSENS MV</div>
                  <p class="toJustify">
                      It is a full body motion analysis system capable of giving 3D joint angles, orientation of bones and centre of mass [2]. The first disadvantage is represented by cost - 35,350 Euro is way beyond our budget. Secondly, it has a small delay in sending data by the method of wireless communication and the use of numerous cables. This would not be practical for the user.  Although, compared with Polhenus it has the advantage that it gives the center of mass information 

                  </p>

                  <div class="subheading mb-2">Notch Sensors</div>
                  <p class="toJustify">
                      Our supervisor Simon suggested these sensors as they are cheap and offer reasonable results when gathering joint position. The problem is that the only way they display real time data is by an android application developed by the company. There is no existing API hence making it hard to program as we require a continuous flow of body segments positions to be used in our own piece software for producing 3D sound.  Being composed from 6 sensors, it would be impossible for us to separately track all 20 joints that we need for calculating the pose of a user. Research also suggests that the sensor interferes with metallic surrounding thus obscuring results [3].

                  </p>

                  <div class="subheading mb-2">Kinect V2</div>
                  <p class="toJustify">
                      Kinect is a depth sensor created by Microsoft that outputs the skeleton position of a user in 3D space using the infrared light and RGB camera. It is a cheap and robust solution, with good documentation and online support.<b>  We decided to go with Kinect,</b> because compared with all previous solutions, it offers the possibility of connecting with Unity and because it offers a perfect balance between cost and performance. This part was vital for us because we needed a game engine or another piece software capable of producing mixed reality applications. Also, it provides a SDK that returns the position of up to 25 joints [4]. The device has very performant audio capturing function as well. This is an important aspect for designing a voice-command based user interface for visually impaired people. However, when compared with the previous solutions it does not offer such good results for occlusion. We can to try to reduce this impact by approximating position and inbuilt functions to filter data from interfered joints.
                  </p>

            </div>
          </div>

          <div class="content-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Potential Frameworks</h3>
              <p class="toJustify">
                  The VirtualTherapy project is a mixed reality application, so we required an editor that would facilitate the construction of such a software. This can done by different <b>gaming engines</b>. They are described below:

              </p>
              <div class="subheading mb-2">Unreal</div>
              <p class="toJustify">
                  Unreal is a powerful game engine and has support for VR/AR applications. However, we have not used it because it does not offer support for integrating Kinect. This is a major downside, since Kinect needs wrapper functions to transform the raw data from the sensors.

              </p>

              <div class="subheading mb-2">Unity</div>
              <p class="toJustify">
                  Unity is a game engine that has a great support for virtual reality applications. The biggest advantage is that Kinect has plugins for Unity. This makes it possible to transfer data from the real world to the virtual world. 
                  
                  <b> We have chosen Unity,</b> because it has great built in functions for creating immersive 3D sound feedback but also an accurate depth imager. The choice was also heavily affected by the large amount of documentation available for Unity and the support given on forums. Problems could be solved more quickly as they may have previously been encountered. 

              </p>

            </div>
          </div>

          <div class="content-item d-flex flex-column flex-md-row">
              <div class="resume-content mr-auto">
                <h3 class="mb-2">Potential Programming languages</h3>
                <b> Front End Development</b>
                <p class="toJustify">
                    <ul>
                      <li> Boo </li>
                      <li> JavaScript</li>
                      <li> C#</li>
                    </ul>

                    
                </p>
                <p class="toJustify">
                    Boo language is like python, which it makes easy to learn and use, but there were few documentations for programming Kinect with Boo.
                    The last 2 are the most popular henceforth <b> we have chosen to program in C#. </b>  Reason being is, most of us are familiar with object oriented programming  languages such as Java, which is similar with C#. If we would have written in JavaScript, there would have been issues with script compilation for premade libraries which we will discuss later below. Apart from this, there is no performance advantages of one language over the other.  

                </p>

                 <b> Server Side Development</b>
                <p class="toJustify">
                  <ul>
                    <li> Node JS</li>
                    <li> PHP</li>
                    <li> Ruby</li>
                  </ul>
                </p>
                <p class="toJustify">
                    <b> For the server part we have used Node JS.</b> Alternatives include Django, PHP, Ruby, but node.js has multiple advantages over all.
                    We have decided to use Node JS due to previous experience. This saved time as we did not have to learn a new backend programming language. It also allows the construction of real time web apps, in which both the client and the server can initiate requests. Being an extension of javascript the documentation provided showed that connection of database drivers were can be easily implemented. Also, it can handle a huge number of simultaneous requests which means high scalability. This is important as Microsoft would like to further develop on the application in the future.
                    
                    </p>
                    
                    

                  <b> Potential Databases</b>
                  <br />

                  <br>
                
                <div class="subheading mb-2"> MySQL Relational Database:
                  </div>
                  <p class="toJustify">
                      MySQL is a powerful database, known for high performance and data security. It is one of the best options for complex queries. However, data must follow the same structure. The structure of our data might change, based on the pose chosen by the user. This means the relationship between joints and their number will differ a lot [7]. Also, we have a lot of relationships between the data which will mean introducing many new tables and linking them together in the most efficient manner. Due to this, running queries on this form of data will be expensive. 
                  </p>
                  <div class="subheading mb-2"> MongoDB
                    </div>
                    <p class="toJustify">
                        This database represents an advantage for us because it allows the structure of data to change. Each pose we save is stored as a JSON styled document, making it easier to process. Also, this database scales horizontally, in comparison with first which scales vertically. This means that more traffic is handled by adding more servers. This is more appropriate for out type of data. Considering this we decided to choose Mongo Db [8].
  
                    </p>
              </div>
            </div>

            <div class="content-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <h3 class="mb-2">Libraries and API's</h3>
             
                  


                    <b> Retrieving Joint Positions From The User:
                      </b>
                      <br clear="all" /><br />

                  
                  <div class="subheading mb-2">	OpenNI  NITE
                    </div>
                    <p class="toJustify">
                       The OpenNI is an API used to initialise devices  that contain depth sensors compatible with Prime-sense form application code. We need to start the depth sensor when the unity application is run, get data from it, and stop it on user’s command [6]. For detecting the human body and accurate joint position from Kinect depth images, the NITE library is used.

                    </p>
                  <div class="subheading mb-2">Unity Kinect MS-SDK</div>
                    <p class="toJustify">
                        This is a library that allows us to map data from Kinect to an avatar that represents the human body in a unity world. It contains wrapper classes that make the transformation, using basic mathematical operations like matrix multiplications. As our application needs to initialise and stop the device, it contains drivers necessary for openNI and the kinect sensor. Also, it provides high level functions for using Kinect features. Currently, it does not have any other competitor libraries that uses Kinect for skeleton tracking <b>hence why we are limited to use this one. </b>
                      </p>

                      <div class="subheading mb-2"> OpenPose plus SMPLify
                        </div>
                      <p class="toJustify">
                          A common problem with this software is the lack of integration with Unity. This is required for the augmented part of our project causing problems as no documentation is provided on how to connect the output of library with the game engine we use. This might be considered as an alternative solution in the future of the project, because SMPLify announced that they are working on creating a Unity plugin. Therefore, the only big problem remaining is connecting the avatar to continuous data. However, this cannot be achieved as SMPLify only works on a single image [10]. It is worth mentioning that this library uses high optimisation suggesting that the computation costs for each image is high. Computing a continuous stream of images would not give real time results as the Lag in the application would be high.
                        </p>
                        <p class="toJustify">
                            However, we have not chosen this solution, because the creation of 3D audio feedback involves applying sound on objects in 3D space. Without the third axis, it is impossible to create objects that mimic our user and apply sound towards it from different directions. A solution to this problem would be the use of SMPLify. This is a machine learning software that can create 3D realistic human models from 2D data.  
  
                          </p>

                          <p class="toJustify">
                              A common problem with this software is the lack of integration with gaming engines. This is required for the augmented part of our project causing problems as no documentation is provided on how to connect that avatar with real time streaming of data. This might be considered as an alternative solution in the future of the project, because SMPLify announced that they are working on creating a unity plugin. Therefore, the only big problem remaining is connecting the avatar to continuous data. However, this cannot be achieved as SMPLify only works on a single image [10]. It is worth mentioning that this library uses CNN (convolutional neuronal networks) suggesting that the computation costs for each image is high. Computing a continuous stream of images would not give real time results.
    
                            </p>
                          <b> Speech Recognition</b>
                          <br clear="all" /><br />

                        

                            <div class="subheading mb-2"> Google Speech API
                              </div>
                            <p class="toJustify">
                                This is a known and powerful API that recognizes around 100 languages. It runs on googles cloud services hence why we considered this as a disadvantage. If no internet connection is present, then the application becomes impractical for visual impaired users, as they have no way of interacting with it. 
      
                              </p>

                              <div class="subheading mb-2"> Microsoft Runtime Speech recognition
                                </div>
                              <p class="toJustify">
                                  We needed a service for speech recognition to implement the voice command functionality.  A disadvantage is that the user must install them on the local device to make our app work. <b> We have chosen this SDK</b> because it runs directly in windows 10. It does not require any calls over the internet so the delay in response time is minimal. This is an important factor for a reliable User Interface [11].
        
                                </p>
                              <b> 3D Sound Effect</b>
                              <br clear="all" /><br />

                              
                              <div class="subheading mb-2"> Oculus Rift Spatializer

                                </div>
                              <p class="toJustify">
                                  This plugin can be used in many game engines and has good documentation. Also, it offers the possibility to customize the effects of the sound. It has the disadvantage that it does not imitate the reflexion of sounds from walls very well. In this case the virtual experience we are trying to achieve will not be immersive. 

                                </p>

                                <div class="subheading mb-2"> Microsoft HRTF (Head related transfer function) spatializer

                                  </div>
                                <p class="toJustify">
                                    Unity already contains this plugin, making it easy to use and configure. <b> We have chosen the Microsoft HRTF</b>, because it offers functions to amplify and reduce noise for different room sizes. Sounds can be heard from left and right directions with walls also reflecting noise signals. Because of this the user can easily locate the source of a given sound. This is a clear advantage over the Oculus rift technology making it suitable to achieve our requirements.

                                  </p>
                    </div>

                </div>
              </div>
              
              <div class="content-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-0">algorithms</h3>
                    <p class="toJustify">
                        Pose prediction algorithms are vital for any VR/AR application that include an avatar. Only using the current position of a user to compute new skeleton tracking images for each frame might result in desynchronization [12].
                        To solve this, we consider some well-known mathematical algorithms. 


                    </p>
                    <div class="subheading mb-3">Exponential Filter</div>
                    <p class="toJustify">
                        This is a smoothing filter used for data when a sudden change appears. It applies one exponential function to smooth the input from Kinect. After a period of experimentation, we had observed that this filter does not perfectly follows the data trend in the abstract movement of joints in motion [13].


                    </p>
                    <div class="subheading mb-3">Double Exponential Filter</div>
                    <p class="toJustify">
                        This method predicts various joint positions using a simple linear regression function. We try to estimate the parameters of the equation when studying the movement of the user. The difference between parameters decreases exponentially over time, prioritising the new data from sensor.  <b> This method is preferred because, </b> by applying the exponential function twice the data follows the trend of the real input [12].


                    </p>
                  </div>
                
                </div>

          <div class="content-item d-flex flex-column flex-md-row">
              <div class="resume-content mr-auto">
                <h3 class="mb-1">Final Decision</h3>
                <p class="toJustify">
                    Our final solution consists of a Kinect V2 Sensor to get joint position from the user, and Unity creating the 3D sound effect. Our language of choice was C# for local application and node.js for the backend programming. We chose to use MS-DK Kinect from the Unity Asset Store with OpenNi to control the Kinect sensor and retrieve data from it. This library also contained wrappers for transforming input into “unity spatial coordinates” allowing us to map to an avatar. We also decided to use the double Exponential Filter to predict the continuous flow data and reduce the phenomenon of occlusion as much as possible.  We also decided to use the Microsoft HRTF spatializer to create the 3D sound effect. 

                </p>
              </div>
            </div>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="hci">
        <div class="my-auto">
          <h2 class="mb-3">HCI</h2>

          <h3 class="mb-2">Sketches</h3>
          <p class="toJustify">
              As part of the HCI requirement we created sketches to visualise how our solution would work.
              However, as the app does provide visual interaction for the user, we have made an overview of the components that it may consist of. 
              The sketches represent how (before realising that we would require a different approach) the sensors will interact with movement.
              
              The initial high level sketch shows the position of all the sensors that will be used to track the position of the body in 3D space, thus allowing us to, in terms of those positions, know whether or not the user is standing using a correct pose or not. The Sensor representation shows how movement will be processed by the sensor, then outputted as useful data that we can then manipulate to make the user stand correctly . Below are the 2 main sketches.  
          </p>
 
          <center>
              <img class="mb-3" src="img/sketch.jpg" width="85%" height="85%">
          </center>


          <br>
          <h3 class="mb-3">Wireframes</h3>
          <p class="toJustify">
              The skecthes then allowed us create a wireframe. It is important to note that this is not the way the solution is implemented, but is the visualisation of the potential way of solving and creating the solution.    
            The Final desgin was crated after conversing with the client and project support. The use of the hololens was appropriate as it eliminated the need of wires.      
          
            In the wireframes shown, we created an initial design that had the sensors outputting their data to an Arduino, which is an open software based on simple hardware and software, ideal for a project such as this, where the amount of data collected isn't too large. This Arduino would then process the data using algorithms that our team has created, to produce 3D sound and guide the user into a standing position. After some communication with the client, we decided to make use of the Hololens as a 3D sound device to output sounds to the user.</p>
          <center>
              <img src="img/wireframefinal2.jpg" width="85%" height="85%">
          </center>

        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="design">
        <div class="my-auto">
          <h2 class="mb-3">Design</h2>

          <h3 class="mb-3">System Architecture Diagram</h3>
          <center> 
          <img src="img/updatedArch.jpg" width="85%" height="80%">
          </center>

          <b> Component A - User</b>
          <p class="toJustify">
              This part of the system architecture diagram shows how the user interacts with the overall system. The headphones indicate the usage of 3D as well as granting the immersive expierence. The user here is intended to be Mark Pollock.

          </p>
          <b> Component B - Kinect Sensor </b>
          <p class="toJustify">
              This represents the Kinect 2 Sensor which retrieves data from component A (the user), using its depth sensor. It captures continuous frames of movement. It must be noted that this piece of hardware must be placed at a suitable height and a distance from the user to fully capture the skeleton image.

          </p>
          <b> Component C - Database Module</b>
          <p class="toJustify">
              It has the role of organising the bones of the avatar in serializable data structures that can be transformed into JSON format. In addition, it has classes that handle the transmission of the JSON to the server component in an asynchronous way.
          </p>
          <b> Component D - 3D Sound</b>
          <p class="toJustify">
              This component represents the output of the previous module and is what the patient hears. It can be modelled separately from the other parts, by choosing different sound scripts and by modifying their settings in the Unity Editor. 
          </p>
          <b> Component E - Avatar and Therapy Module</b>
          <p class="toJustify">
              This is the core module of our application. It has the role of analysing the posture of the avatar and give feedback based on that. Furthermore, it coordinated which sound scripts to play and when allowing the correction of one joint at a time. In this way, we avoid a mixture of different noises that would not tell anything useful for the user.
              </p>
            <b> Component F</b>
            <p class="toJustify">
                This part receives the request from the main application and saves the JSON documents in the database. It acts as a secure bridge between the app and database. By running on the server it makes difficult for third-party  to access the security information about the database.              </p>
              <b> Component G - Database</b>
              <p class="toJustify">
                  We used the database to store the poses a user might want to achieve during the exercises. It contains just the positions and rotations of the user in relation to the avatars bones 
                </p>
                <b> Component H - Kinect Module</b>
          <p class="toJustify">
              This section gets the data from the sensor and transforms it to the Unity format. Also, we map it or our avatar so that it can mimic the patient’s movements.
            </p>
          <h3 class="mb-3">Class Diagram</h3>
            <center>
                <img src="img/ClassDiagramNormal.jpg" width="90%" height="90%">
            </center>
          
            <h3 class="mb-3">Sequence Diagram</h3>
            <center>
                <img src="img/sequenceDiagram.jpg" width="90%" height="90%">
            </center>

          <h3 class="mb-3">Design Patterns used</h3>
          <b> Observer Pattern</b>
          <p class="toJustify">
              This pattern is used to notify the observers when an event has occurred. We create loosely coupled code by just notifying the listeners when a changed has occurred, without calling specific methods. In our implementation, we used an Observer pattern to update the avatar when a changed is seen in the patient’s position. Also, we needed it for classes that wait for user’s command in order to execute some code. Once a word is recognised, the state of the subject changes and all its observers are alerted.

          </p>

          <b> Command Pattern</b>
          <p class="toJustify">
              This pattern was used to separate the invoker of an action from the receiver. The code is separated such that the object that calls the command has no idea about the specific implementation of it. An concrete example is the voice command menu. When we require to save something to the database, the code that recognises the user’s voice will invoke the “save pose” function call on the Button Manager class. This will call the execute method of “DataSender” class that contains the code for sending information to the server. 
          </p>

          <b> Singleton</b>
          <p class="toJustify">
              A singleton was used when it was unnecessary to have more objects of the same type. For example, having multiple speech managers on runtime would mean having more calls on code that executes commands, while the user wanted to perform the action only once. Despite this, the design pattern has a clear scope, it is dangerous and not beneficial to overuse it - For example it can break thee SOLID principles of object-oriented design. Also, if not used adequately it can cause numerous bugs, especially in multithreaded applications. </p>

          <b> Strategy Pattern</b>
          <p class="toJustify">
We had used this as a stronger alternative to the Template method pattern because it allows the code to be more flexible as it respects the open-closed principle. This means that future developers should just add new implementations that obey the current interfaces, instead of changing the current work.  This considerably reduces the maintenance costs. The pattern was applied to the algorithm that recognises different poses and to the part that chooses which sound is played, making it possible to easily change the approach, if advanced solutions are found in the future.  
          </p>

          <b> Adapter Pattern</b>
          <p class="toJustify">
            The adapter was required for connecting the Sensors raw data format into Unity spatial coordinates. We used some existing wrapper classes, which are the Adapters to call a specific function on the Kinect interface and change the returned result in a way that fulfilled our needs.
          </p>
          <b> Decorator Pattern</b>
          <p class="toJustify">
              This pattern allowed to regain more control on sounds that are played during runtime. The simple Unity behaviours “Play ()” and “Stop ()” were not enough to coordinate multiple sound sources on the same Game Object. We had to create a decorator class called “Sound Settings” that would add extra functions to the original methods of Unity’s AudioSource component. Doing this we avoided direct modifications of existing work, which would have been impossible. 
          </p>
          <b> Dependency Injection Pattern</b>
          <p class="toJustify">
              TDD (test driven development) made use of loosely coupled code. We had to use the dependency injection to avoid creation of an object in concrete classes. Instead, we passed references in the constructors, making the code more flexible, reusable and testable.
          </p>
          <h3 class="mb-3">Data Storage</h3>
          <p class="toJustify">
              Although this was not one of the main requirements, we needed a someway to store different postures that user might want to achieve during the exercise. We used the Mongo database which was deployed on mLab (a cloud service for mongoDB)  because of its fast and easy configuration. To connect with it we created a server file in node.js that was deployed on Azure. For deploying we created a GitHub webhook that would put the new code on the cloud each time a pushed was made on the production branch. 
          </p>

          <center>
              <img src="img/databaseSchema.jpg" width="70%" height="70%">
          </center>

          <h3 class="mb-3">Key functionalities </h3>
          <b> Gather 3D joints from the environment and map them to an Avatar
            </b>
            <p class="toJustify">
                In order to track the movement of the user’s body in real time we used the Kinect device and the Kinect SDK created by Microsoft. It must be mentioned that the tracking algorithm implemented in the sensor works with face detection, which means that the patient must face the Kinect to have his limbs position monitored. Once data is received from the device, we must process it and transform from the raw format into Unity spatial coordinates. This transformation is done using wrapper classes from the MS-SDK library we imported. 
                <br>
                Furthermore, we filtered the converted data using the double exponential filter method, described in the research part. The process of filtering is necessary because occlusion (misalignment of joints) can occur, meaning that Kinect gives inaccurate results when compared with real coordinates. Also, an approximation might be needed when some joint changes its position between different frames. If no approximation is done, a sudden difference in avatar’s position will be noticeable, making it desynchronized from the user’s movements.
              <br>
              After correctly configuring the above steps, we must map the retrieved information to game objects which are represented as bones of the avatar. All virtual bones are arranged into a hierarchy which denotes the human skeleton.

            </p>
            <b> Develop an algorithm that detects incorrect pose of the user
              </b>
            <p class="toJustify">
                This requirement had been quite challenging due to the numerous ways of tackling this problem. We required a unique which applied to a simplified version of our task. There are many machine learning algorithms that can indicate and perceive the pose of the user, but are computationally expensive. This factor is an obstacle for giving real time feedback. To avoid this problem, we designed a rule-based system, that gives feedback on the pose by considering the relation between multiple joints. For example, to determine how bent the left knee is, we calculate the angle between 3 points: left hip, ankle and knee. Those joints form 2 vectors in 3d space, which makes it easy to calculate the angle using this formula:
                <br>
               <center><b> <math xmlns="http://www.w3.org/1998/Math/MathML"> 
                <mi>𝜃
                    </mi><mo>=</mo><msup><mi>cos</mi><mrow><mo>-</mo><mn>1</mn></mrow></msup>  <mfrac> <mi> |P|  x  |Q|</mi><mn>P x Q</mn> </mfrac></mfrac> </math> </b> <i> where P, Q are the vectors defined by the joint points</i></center> 
               
                <br>
                We know that the knee has a correct position when the value of the angle is inside an interval of error.<br>
                    
  <pre> 
  <code class="language-clike"> 
float angleKnee = calculateAngle3D (hip, knee, ankle);
  if (angleKnee >= 15f) {
    percentError = calculateError (15f, angleKnee);
  }  </code> </pre>     
                         
                As the definition of correct joint alignment depends on the user’s preferences, we define the conditions as variables which are calculated from the predefined choice of the avatar model. In our case the clients clearly defined what a correct pose means. Below is an example of what a imbalanced and balanced skeleton looks like.
                <center>
                    <img src="img/keyfunc.jpg" width="60%" height="60%">

                </center>
                Different body parts can affect human position independently of the other limbs, which is the case of spine base. The solution is to simply take their rotation in space.   Despite there are distinct conditions, our system can handle all different approaches, because there are just a limited number of possibilities to consider. 
              </p>

                <b> Transform feedback in 3D sound modulation
                </b>
                <p class="toJustify">
                    As our end user was visual impaired, he required audio guidance to know how to correctly align his joints. Simple verbal instructions were not enough, because they could not reflect how the user was progressing in terms of his current state. In other words, the verbal instructions did not suggest when knees are bent at an angle of 5 degrees and when at 60 degrees. There was not differentiation between the two. Instead, we took the approach of trying to localise the joints with 3D sound. In real life, we can distinguish sources of noise by how far they are and from which direction.  To avoid an unnecessary large learning curve, we can allocate keywords to each joint. This means we can combine the 3D sound affect principal along with this to create a tailored experience for the user.
                    To accomplish an immersive experience, we used the Microsoft’s HRTF spatializer (Discussed in the research part). 
                    <br>
                    <br>
                    As the role of avatar has been clearly defined, we need to create the 3D sound effects that represent the source of noise to be present in the scene. Taking the head of the humanoid (The avatar) as listener, position of those objects can be easily deduced from sound modulation.  The patient will have the feeling of being substituted in place of the avatar in the virtual world hence being be able to realise where his joints are localised in space.
                    <br>
                    <br>
                    Despite the solution being good, we faced numerous challenges.  Firstly, the spatializer was built to imitate reality as much as possible. This meant that large distances have a noticeable effect on the sound volume. In our case, even 5 degrees of motion were extremely important and needed to be signalled to be changed. To solve this, we created mock joints. In the Unity scene, these move exponentially in relation to the user’s joints on a specific axis. For example, if the left hip is not properly aligned with the rest of the body (pushed too far on the left) than there is a difference <b> <i> Dx</i></b> between the correct location and the current location on the x axis. Considering that value of <b> <i> Dx</i></b> is constrained by the human body shape and is rigid (i.e it belongs to [-1, 1]) we can compute a new position for a mock hip by applying the function
               <b> <math xmlns="http://www.w3.org/1998/Math/MathML">
                    
                      <mi>Exp(Dx)
                        </mi><mo>=</mo>  
                       <msup><mi>e</mi><mi><mi> 10x</mi><mi> D</mi><mi>x</mi></mi></msup>
                    </math></b>
                    
                    
                    . 
                    The newly created point will have the same values on y and z axis as the human joint while the x value will be given the previously mentioned function. The new object will then have a sound source attached to it. The schema below explains the concept:
                   <center>
                      <img src="img/keyfuncscheme.jpg" width="60%" height="60%">
                   </center> 
                   Secondly, a wrong pose can be the result of multiple joints that need correction. In this case, we cannot play at once all the various noises mapped to those joints. This will cause large amount of confusion for the user. Instead we try to choose the correct joint that has the maximal error. However, different limbs have different acceptance intervals of distinct ranges. We solved the issue by considering the proportions of joints and limbs and calculating the percentage error obtained from each proportion. 

                </p>
                <b>Main Menu with Voice activated Commands
                  </b>
                  <p class="toJustify">All users should be able to interact with the application in a friendly and easy-to-use manner. Since our target audience are visually impaired people we had agreed with the client to construct a menu based on speech recognition. Our solution utilises a small grammar of several instructions combined with the speech recognition features of Windows 10. Furthermore, Kinect has a very performant microphone that allowed us to create a reliable user interface. The only problem with the speech recognition system is its vulnerability to external sounds. This is the case for all speech engines.
                    </p>
        </div>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="testing">
        <div class="my-auto">
          <h2 class="mb-3">Testing</h2>
          Various testing stratigies were used to ensure that functionalities operated smoothly. We will discuss the various testing techniques below and the situation in which they were performed.
          <br clear="all" /><br />

          <b> Functional Testing</b>
          <p class="toJustify">
              This testing strategy is used to ensure that every function of the system works in relation to the initial requirements. The main technique used is called black-box [16] testing which does not involve any source code. The test is run on the  input requirement which is then checked against a test with a defined expected output.
              <br>We performed functional testing using a step by step procedure [16]:
              <ol>
                <li> Identify the requirements. We had done this at a previous phase of our project iteration
                  </li>
                <li> Create appropriate input data for the test cases. 
                    Because the combination of joint alignments is a large number, we had to identify which joints are the most common. We then stored their positions and provided an input for our test
                    </li>
                    <li> Determined the output depending on how functionality was defined:
                        We had to consult with Dr Sayenko to find out what are the possible ways a patient tries to correct the pose. This is because the order of joints that are corrected affects the output of our application. In addition, we had a clear vision of how a corrected posture would like due to numerous previous meetings with our supervisors. </li>
                      <li> Run the test:
                          Depending if our test was automatic or manual, we executed the test cases and obtained an output
                          </li>
                        <li> Compare the output against the expected values:
                            In this section we could define if the test was failing or not, giving us an idea of what was wrong with our software. 
                            </li>
              </ol>
              
          </p>

          <b> How we approached Testing</b>
          <p class="toJustify">
              By now we had tested the functional requirements (described in the Requirement section) and the UI. We mainly used the black box technique as we were interested in how certain interfaces behave and not how they are implemented. When we discovered a test that failed and was hard to debug we had to refer to the Transparent technique (White box testing) and investigate the internal behaviour of the code. The main argument for not using it as much - is because the first method tends to focus on the implementation details, rather than of the main functionality. 
              In the near future, we plan to test the application with our end user. His location in Ireland prevented us to do so. However, we discussed it with our clients and hope to ship the software to him or find a way to arrange a meeting.<br>
              Different Companies have their own way of defining the next test types, so we follow the ISTQB (International Software Testing Qualifications Board) definitions.  
              
          </p>

          <b> System Testing</b>
          <p class="toJustify">
              We had to test our application from one end to another, so after we finished the requirements, Unit and Integration tests we performed this type of testing. In Unity and other Game Engines, it is very hard to automate testing, because you need to check interactions between objects. Unity provides a Test Runner tool, but it is appropriate for only Unit and Integration Tests. To automate the verification of the entire app we had to recreate the scenes from runtime, which would have been time consuming and inefficient. Instead, we manually checked the behaviour running through all features once.  When the found faults were fixed we repeated the cycle, gradually improving the software.
          </p>

          <b> Stress Testing</b>
          <p class="toJustify">
              Stress testing is a non-functional technique that is used to observe specific results when nearing a breaking point. It is a subset of performance testing thus allowing us to monitor and analyze any failures that occur. In regards to the project it used to verify if the system handles the post request from the unity application and successfully stores the data into a database. This was done using VSTS (Visual Studio Team Services)
            <Br>
              <br>
              Each test case has 4 different graphs named as: Performance, Throughput, Errors and Tests. The userload is static through out with the test duration lasting 1 minute. The performance graph indicates the average response time along with the throughput which shows how many requests it is receiving within a second. The error and test graph are self explanatory.
          </p>
          <div class="subheading mb-3"> Post request to server hosted on Azure </div>

          <center> <img src="img/postReqServer.jpg" width="90%" height="90%"></center>
          <div class="subheading mb-3"> Post request to server hosted locally</div>

          
          <center> <img src="img/postReqLocal.jpg" width="90%" height="90%"></center>
          <p class="toJustify">
              On the Unity side, we test how the program detects normal movements of the joints at a constant rate. Furthermore, we examined how the speech recognition API behaves with different voices or accents
          </p>

          <b> Load Testing </b>
          <p class="toJustify">
              Load testing is another type of performance test that was carried out. It was used to determine the QOS (quality of service) based on large amount of user requests. We created virtual users which simultaneously accessed the URL. It granted us critical information such a the average response time and the number of errors generated. It increased our confidence in the scalability and reliability of the system as it could handle large amount of requests. The results below show a total of 2923 requests being received over a span of two minutes.
        </p>

        <div class="subheading mb-3"> URL receiving large amount of requests</div>
        <center> <img src="img/loadUrlLocal.jpg" width="90%" height="90%"></center>

        </div>

          <b> Unit Testing</b>

          <p class="toJustify">
              We followed the TDD (test driven development) approach to design our application. We first defined unit tests to check each behaviour and then created code to pass the tests. By doing so we developed code with fewer bugs and better design. The tests were automated from the beginning using the NSubstitute [17] framework and Unity Test Edit Mode. [18] 
              To test the components of the node.js part we used Google’s ARC tool to examine how it handles post requests.
          </p>
          <center> 
              <img src="img/UnitTestsUnity.jpg" width="50%" height="50%">
            </center>
          <p class="toJustify">
              Unit tests were also implemented on the nodeJS file to ensure correctness and robustness. Two main libraries were used which are Chai and Mocha. Chai is an assertion library which checks if the given input matches the pre-defined output. It follows the style of test driven development. However Mocha is javascript framework allowing one to create asynchronous tests which run serially. Below are example test cases which were run on the server file. It shows the Test, its expected result and whether it failed or not (Indicated by a green tick and Red cross).          </p>
          <center> 
              <img src="img/nodeTest.jpg" width="50%" height="50%">
            </center>

            <b> Integration Testing</b>
            <p class="toJustify">
                We used integration tests to see how Kinect module binds with the component that gives feedback based on the pose. In addition, we had to check the integration with the database. The Bottom-Up method was used to code the lower level components first and then join them together to form bigger clusters. Those combined components were tested which allowed it to move upwards in the hierarchical structure of the program, creating a fully functional app.
            </p>

            <center>            <img src="img/IntegrationTests.jpg" width="50%" height="50%">
            </center>

          </center>


          <b> Acceptance Testing</b>
          <p class="toJustify">
              Since we could not reach our end user we had to test the app on our colleagues and friends.
          </p>
          <center>



          
          <table id="testingTable"class="table-md table table-bordered" style="font-size:14px">
              <thead class="thead-light">
                <tr  align="center"><th>Test ID</th><th>Test Scenario</th></tr>
              </thead>
              <tbody>
                <tr>
                  <td align="center">
                    Test 1
                  </td>
                  <td>
                     Start Pose Correction                       
                       </td>

                </tr>

                <tr>
                    <td>
                      Test 2
                    </td>
                    <td>
                       Exit the application                    
                        </td>

                  </tr>
                  <tr>
                      <td>
                        Test 3
                      </td>
                      <td>
                         Save Pose                 
                          </td>
  
                    </tr>
                    <tr>
                        <td>
                          Test 4
                        </td>
                        <td>
                           Calibrate Headphones                   
                            </td>
    
                      </tr>
                      <tr>
                          <td>
                            Test 5
                          </td>
                          <td>
                             Repeat Insutructions                  
                              </td>
      
                        </tr>
                        <tr>
                            <td>
                              Test 6
                            </td>
                            <td>
                             Navigate Menu                
                                </td>
        
                          </tr>

                          <tr>
                              <td>
                                Test 7
                              </td>
                              <td>
                                Left hip wrong position             
                                  </td>
          
                            </tr>
                            <tr>
                                <td>
                                  Test 8
                                </td>
                                <td>
                                   Right knee wrong position                   
                                    </td>
            
                              </tr>

                              <tr>
                                  <td>
                                    Test 9
                                  </td>
                                  <td>
                                     Pose Correct achieved                   
                                      </td>
              
                                </tr>
                </tbody>
                </table>
             
            </center>

            <div class="md-4 row">
       
                <!-- <div class="col-sm-1 text-center sr-button"></div> -->
                <div class="col-sm-6" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                  
                  <img src="img/test1.jpg"width="90%" height="80%">
                  <br><br>
                    
                </div>
                <div class="col-sm-6" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
              
                    <img src="img/test2.jpg" width="90%" height="80%">
                     <br><br>

                  
                </div>

                </div>
                <div class="md-4 row">
       
                    <!-- <div class="col-sm-1 text-center sr-button"></div> -->
                    <div class="col-sm-6" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      
                      <img src="img/test3.jpg"width="90%" height="80%">
                      <br><br>
                        
                    </div>
                    <div class="col-sm-6" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                  
                        <img src="img/test4.jpg" width="90%" height="80%">
                         <br><br>
    
                      
                    </div>
    
                    </div>

                    <div class="md-4 row">
       
                        <!-- <div class="col-sm-1 text-center sr-button"></div> -->
                        <div class="col-sm-6" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                          
                          <img src="img/test5.jpg"width="90%" height="80%">
                          <br><br>
                            
                        </div>
                        <div class="col-sm-6" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                      
                            <img src="img/test6.jpg" width="90%" height="80%">
                             <br><br>
        
                          
                        </div>
        
                        </div>         
                        <div class="md-4 row">
       
                            <!-- <div class="col-sm-1 text-center sr-button"></div> -->
                            <div class="col-sm-6" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                              
                              <img src="img/test7.jpg"width="90%" height="80%">
                              <br><br>
                                
                            </div>
                            <div class="col-sm-6" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                          
                                <img src="img/test8.jpg" width="90%" height="80%">
                                 <br><br>
            
                              
                            </div>
            
                            </div>

                            <center>
                                <img src="img/test9.jpg" width="40%" height="40%">
                              
                            </center>
                            <br>
                            <h3 class="mb-3">Client and User Feedback</h3>
                            
                            <b> From The Client:</b>
                            <p class="toJustify">
                              "Rares, please pass the following onto the rest of the team From my perspective, I think you and the team, but you in particular, as the leader of your group, have done an outstanding job. You were set a very difficult and challenging project, something new, something exploratory... and not only did you engage really well and sensitively with your key client contacts Mark and Dimitry, but the way in which you absorbed the information they provided and rose to the challenge to create a working prototype has been fantastic. You handled the technical challenges extremely well... reached out with questions and concerns when you had them, made sure there was a regular stream of project updates and statuses, which from a communication point of view is really important... because it can be all too easy to bury your head in the technology and lose sight of the impact on a person’s life you are trying to have. To the great credit of you and your team, your passion and commitment has been unrelenting over the course of this project. You set yourselves a high and challenging goal, and have made tremendous progress which I think has exceeded what we thought was possible given the challenges you were dealing with "

                            </p>

                            <b> From The User</b>
                            <p class="toJustify">
                              “The complexities of multiple sensory impairment are difficult to understand for most people. Dolga and his team managed to appreciate those complexities and apply logical thinking to the problem. Both the concept and practical solution developed are way beyond what I expected. I believe that this solution will be a significant step along the path towards a cure for paralysis.”  
Mark Pollock – Explorer & Collaboration Catalyst at the Mark Pollock Trust"
                            </p>

                            <b> From Michael</b>
                            <p class="toJustfy">
                             "  Passion, seeking to understand, research, attention for detail, keeping us informed and engaged every step of the way, going beyond in helping us to next steps beyond the current project.
I loved the outcome and I want to make sure that we can give it due attention for next steps.
I can see it helping with the original use case, but also in other applications, such as vestibular rehabilitation, proprioception, convalescence,…"
                            </p>


        <b>Compatibility Testing  </b>
        <p class="toJustify">
            Compatibility testing was carried out to ensure the application did not have any discrepancy when working on a different OS.As the application is only intended for a windows platform we decided to test it on two of the main releases. These are windows 8.1 and windows 10. The results are shown below.        </p>
        <center>
          <div class="doTable">

          
            <table class="table-md table table-bordered" style="font-size:14px">
                <thead class="thead-light">
                  <tr><th>Operating System</th><th>Version</th><th>Architecture</th> <th>Result</th></tr>
                </thead>
                <tbody>
                  <tr>
                    <td>
                      Windows
                    </td>
                    <td>
                       8.1                            </td>
                    <td>
                      64 bit
                    </td>
                    <td>
                      Successful
                    </td>
                  </tr>

                  <tr>
                      <td>
                        Windows
                      </td>
                      <td>
                         10                           </td>
                      <td>
                        64 bit
                      </td>
                      <td>
                        Successful
                      </td>
                    </tr>
                  </tbody>
                  </table>
                </div>
        </center>

        <b> Responsive Design Testing</b>
        <p class="toJustify">
            We tested our app on two main devices which have different features (e.g. screen size, memory). We had  to make sure our app responds well to these changes. The results are summarised within the table below.
        </p>
        <center>
            <div class="doTable">
  
            
              <table class="table-md table table-bordered" style="font-size:14px">
                  <thead class="thead-light">
                    <tr><th>Device Model</th><th>Resolution</th><th>RAM Memory</th> <th>Processor</th> <th> Result</th></tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>
                        Lenovo G-50
                      </td>
                      <td>
                         1600 x 900p                           </td>
                      <td>
                       4
                      </td>
                      <td>
                          Intel® Core™ i7-8700K Processor
                      </td>
                      <td> Successful</td>
                    </tr>
  
                    <tr>
                        <td>
                          Dell 8080
                        </td>
                        <td>
                           2560 x 1440p                           </td>
                        <td>
                          8
                        </td>
                        <td>
                            Intel® Core™ i5-8600K Processor
                        </td>
                        <td>
                          Successful
                        </td>
                      </tr>
                    </tbody>
                    </table>
                  </div>
          </center>
          <b> Automated and Continous Integration Testing</b>
          <p class="toJustify">
              As previously mentioned in Unit and Integration Testing we used NSubstitute to create automated tests. In addition, we wanted to have automatic builds as well. To achieve this, we used Travis CI, a continuous integration tool. We linked GitHub project with our Travis account and created bash scripts to build and run tests every time we pushed to GitHub. If the build or at least one test failed a notification email with details is sent to the developer who pushed to the branch.  After fixing the failed test we reran all the tests to ensure that no new bug were introduced.
          </p>
          <center>
            <img src="img/automated.jpg" width="80%" height="80%">

          </center>
      </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="evaluation">
          <div class="my-auto">
            <h2 class="mb-3">Evaluation</h2>
            <div class="content-item d-flex flex-column flex-md-row mb-3">
              <div class="resume-content mr-auto">
                <h3 class="mb-0">Bug Table</h3>
                <center>
                  <img src="img/bug.jpg" width="80%" height="90%">

              </center>
              </div>
            </div>

            <div class="content-item d-flex flex-column flex-md-row mb-3">
                <div class="resume-content mr-auto">
                  <h3 class="mb-0">Achievement Table</h3>

                </div>
              </div>
              <center>
                  <img src="img/achievement.jpg" width="60%" height="60%">

              </center>


                <div class="content-item d-flex flex-column flex-md-row mb-3">
                    <div class="resume-content mr-auto">
                      <h3 class="mb-0">Contribution Table</h3>


                     
                    </div>
                  </div>

                  <center>
                    <img src="img/contribution.jpg" width="80%" height="90%">

                  </center>

                  
                  <div class="content-item d-flex flex-column flex-md-row mb-3">
                      <div class="resume-content mr-auto">
                        <h3 class="mb-0">Critical Evaluation</h3>
                        <p class="toJustfy">
                            The user interface and their experience is one of the most important things to bear in mind when developing an application, because it is the part they interact with and see. The user interface and overall feel of the application can determine whether the application is ultimately successful or not. During the course of the project there have been several changes to the user interface and experience, these have been both stylistic and structural changes. Our iterative design directed us to  choosing a vocal user interface specially designed for visually impaired users.
                        </p>
                        <b> Functionality</b>
                        <p class="toJustify">
                            The user interface is what the user sees and interacts with. Functionality is what the user is able to do with the application. This makes it an equal, if not more important, aspect to consider when developing an application. The main purpose of our application is to correct the posture of paralysed patients during therapy sessions. This was achieved by breaking down the overall functionality into several subsections. First we had to integrate the Kinect sensor with Unity and process the input data. The pose analysis algorithms we developed were also an important aspect of the application’s functionality. Lastly, generating 3D sounds using the output of the  previously mentioned algorithms was a core functionality, that was successfully integrated for a more immersive experience.  Since we approached an open research problem, our solution is specific for our scenario and subject area, thus, it is not perfect. Voice Command features are another key aspect of our functionality that we added as an extra requirement. 
                        </p>

                        <b> Stability</b>
                        <p class="toJustify">
                            The application we developed is stable and does not crash. Testing has ensured an error free software. Depending on the hardware components of the computer used, the application can run slower than normal, but it is still stable. Regarding reliability, our application gives thoroughly reliable feedback for pose correction. The only problem, as explained in previous sections, is occlusion. If this phenomenon appears, then the output from our algorithms is not completely correct, due to the input data from sensors not being accurate.

                        </p>
                        
                        <b> Efficiency</b>
                        <p class="toJustify">
                            VirtualTherapy was designed to be as efficient as possible, especially since we have to give feedback in real time. We could not afford to use time expensive methods for pose recognition and 3D sound generation. Furthermore, we optimized the algorithms we used as much as possible, both from a time and memory perspective. 
                        </p>

                        <b> Compatibility</b>
                        <p class="toJustify">
                            Our software was designed using Microsoft technologies and is therefore targeted for their operating system, meaning that Windows 10 is the only OS on which the application will run. In terms of hardware, the computer running the application should have at least 4GB of RAM and a USB3 port. 
                        </p>

                        <b> Maintainability</b>
                        <p class="toJustify">
                            We tried to develop the code following the SOLID design principles. This implies that our code is flexible, reusable and maintainable. By separating the code in multiple components we created a structure that is easy to understand. Furthermore, it is easy to  change some components like the Kinect section, as technology is rapidly improving. 

                        </p>

                        <b> Project Management</b>
                        <p class="toJustify">
                            We organised our project based on 3 major deadlines. The first was on the 12th December 2017, by which we had to complete the research phase of our project and propose a credible solution. The second point in our timeline was the 3rd of March 2018, on this date we had to present a prototype of our work. Finally, the last major due date is the 22nd of March 2018, whereby a complete version of the application needs to be submitted. Along with these checkpoints, we created internal deadlines to assure we would deliver the product on time. We created a Gantt chart that shows how we organised our time. 
                        </p>

                      </div>
                    </div>
    
                    <div class="content-item d-flex flex-column flex-md-row mb">
                        <div class="resume-content mr-auto">
                          <h3 class="mb-0">Future Work</h3>
                          <p class="toJustify">
                              The  project could be improved in various ways if an extra 3 months were given. The quality of the sounds used would be adjusted in terms of replicating natural audio. With regards to the avatar, we would remove the anime-type avatar and replace it with a realistic representation of a human avatar. Although this would not benefit the user, it would provide the application with a more professional feel. One of the main problems we had faced was the obscurity caused by occlusion. With extra time given we could improve the way we deal with occlusion. One possible solution would be to implement deep learning. However, the deep learning algorithm should be developed from scratch and use RGB depth images as input. Another way we could take advantage of those 3 months would be if a more cost-effective sensor were to become available. In such a case, a more efficient and accurate sensor would give us more accurate data from the user, creating a more effective application. Although not required by the client, a possible addition to the application would be to add a wider variety of physiotherapeutic exercise, such as taking a step forward or backwards. This would add more functionality to the application, opening it up to a larger user base. 
                          </p>
                        </div>
                      </div>

          </div>
        </section>

        <section class="content-section p-3 p-lg-5 d-flex flex-column" id="management">
            <div class="my-auto">
              <h2 class="mb-5">Management</h2>

              <div class="content-item d-flex flex-column flex-md-row mb-3">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-0">User Manual</h3>
                    <p class="toJustify">
                      The screenshots and description below them show how the application is run. As the user is visually impaired, these steps demonstrated will be for the physiatrist/trainer to undertake.
                    </p>

                    <b> Getting the Application running</b>
                    <ol>
                      <li> Run the executable file called with the unity logo. This will prompt you to the next screen.
                        </li>
                      <li> Select from the given screen sizes and then click play. The unity application will start running. A blue screen with menu user interface will appear</li>
                      </ol>
                      <b> Step 1: Calibrating the headphones</b>
                      <ol>
                        <li> When the application is directed towards to the main scene, it will vocally walk through the options of what the user must say.</li>
                        <li> The visually impaired user will say "Calibrate"</li>
                        <li> Headphones can now be adjusted based on which side the sound is emitting from.</li>

                      </ol>
                      <b> Step 1.1: Repeat Instructions</b>
                      <ol>
                        <li> If the user has forgotten which command he intends to run, or did not understand the audio, he can simply say "Run"</li>
                        <li> The instructions will then be repeated.</li>
                      </ol>

                      <b> Step 2: Start Pose Correction</b>
                      
                      <ol>
                        <li> The user says "Start"</li>
                        <li> A replication of the user is modelled into an avatar with another screen in the bottom left showing the user himself.</li>
                        <li> Sounds are initially played indicating to the user which sound is related to which joint. </li>
                        <li> The algorithm has now begun. Sounds increase in intensity if the user is distant from the correct posture. Conversely, they decrease if the current state of the user is improving.</li>
                      </ol>
                      <b> Step 3: Exit Application</b>
                      <ol>
                        <li> From the Start Pose screen, the user must vocally say "Main Menu" to return to the initial screen</li>
                        <li> The user will now say "Exit"</li>
                      </ol>

                    </ol>

                    <!--
                    <div class="md-4 row">
       
                        <div class="col-sm-4 text-center sr-button" data-sr-id="1" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                          <h6>Step 1<br>
                          </h6><h3>
                            <img src="img/startPhase.jpg" width="70%" height="70%">
                          </h3>
                          <br>
                            <medium>
                              Run the executable file called with the unity logo. This will prompt you to the next screen.
                            </medium><br>
                            <small></small>
                            
                        </div>
                        <div class="col-sm-4 text-center sr-button" data-sr-id="2" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                          <h6>Step 2<br>
                            </h6><h3>                            
                              <img src="img/playScreen.jpg" width="70%" height="70%">
                            </h3>
                             <br>
                            <medium>Helps the user to correct his/her position through pleasant sound iterations.
                              For Example trying to stand straight. 
                            </medium><br>
                            <small></small>
                          
                        </div>
                        <div class="col-sm-4 text-center sr-button" data-sr-id="3" style="; visibility: visible;  -webkit-transform: translateY(0) scale(1); opacity: 1;transform: translateY(0) scale(1); opacity: 1;-webkit-transition: -webkit-transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; transition: transform 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s, opacity 1s cubic-bezier(0.6, 0.2, 0.1, 1) 0.2s; ">
                          <h6> Step 3<br>
                            </h6><h3>                              
                              <img src="img/startNoSpeech.jpg" width="70%" height="70%">
                            </h3>
                            <br>
                            <medium> Store the current posture into a database.</medium><br>
                            <small></small>
                          
                        </div>
                        
                        </div> -->
                        
                  </div>
                </div>

              <div class="content-item d-flex flex-column flex-md-row mb-3">
                  <div class="resume-content mr-auto">
                    <h3 class="mb-2">Deployment Manual</h3>
                    <ol>
                      <li>
                          	Choose a computer that has a USB 3 Port and 4 GB RAM memory and runs Windows 10

                      </li>
                      <li>
                          Install Kinect SDK: <a href="https://www.microsoft.com/en-us/download/details.aspx?id=44561" target="_blank"> Click here</a> 

                      </li>
                        Install Microsoft runtime speech recognition: <a href="https://www.microsoft.com/en-us/download/details.aspx?id=27225" target="_blank"> Click here </a>

                      <li>
                          Install Kinect Language Pack: <a href="https://www.microsoft.com/en-us/download/details.aspx?id=43662"> Click here</a>

                      </li>
                       To play the application just click SoundTherapy.exe file

                      <li>
                          If you want to open it in the development environment - Unity, choose Open Project and select the “VirtualTherapy” folder

                      </li>
                    </ol>

  
                  </div>
                </div>



                <div class="content-item d-flex flex-column flex-md-row mb-3">
                    <div class="resume-content mr-auto">
                      <h3 class="mb-0">Gantt Chart</h3>
                    

                     <center>

      
                      <img src="img/ganttchart2.jpg" width="95%" height="90%">
                    </center>
                    </div>
                  </div>

            </div>
          </section>

      <section class="content-section p-3 p-lg-5 d-flex flex-column" id="References">
        <div class="my-auto">
          <h2 class="mb-3">References</h2>
          <ul class="list-unstyled">
            <li>
                [1] Polhemus Fast-Track: Available at: <a href="https://polhemus.com/motion-tracking/all-trackers/fastrak#collapseOne">https://polhemus.com/</a>

            </li>
            <li>[2] XSENS: Available at: <a href="https://www.xsens.com/tags/sports-biomechanics/">https://www.xsens.com/tags/sports-biomechanics/</a>
              </li>
              <li>[3] NOTCH: Available at: <a href="https://wearnotch.com/">https://wearnotch.com/</a>
                </li>
                <li> [4] Kinect: Available at: <a href="https://msdn.microsoft.com/en-us/library/dn758675.aspx">https://msdn.microsoft.com/en-us/library/dn758675.aspx</a>
                  </li>
                  <li> [5] Paper Kinect: Available at: <a href="https://www.cla.purdue.edu/academic/vpa/ad/act/resources/ad41700_barrett_kinect_unity.pdf">https://www.cla.purdue.edu/academic/vpa/ad/act/resources/ad41700_barrett_kinect_unity.pdf</a>
                    </li>
                    <li>[6] OpenNi: Available at: <a href="https://s3.amazonaws.com/com.occipital.openni/OpenNI_Programmers_Guide.pdf">https://s3.amazonaws.com/com.occipital.openni/OpenNI_Programmers_Guide.pdf</a>
                      </li>
                      <li>[7] MySQL: Available at: <a href="https://dev.mysql.com/doc/refman/8.0/en/">https://dev.mysql.com/doc/refman/8.0/en/</a>
                        </li>
                        <li>
                            [8] Mongo Db: Available at: <a href="https://docs.mongodb.com/manual/">https://docs.mongodb.com/manual/</a>

                        </li>
                        <li> [9] OpenPose: Available at: <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/README.md">https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/README.md </a>
                          </li>
                          <li> [10] SMPLify: Available at: <a href=" http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf"> http://files.is.tue.mpg.de/black/papers/BogoECCV2016.pdf</a>
                            </li>
                            <li> [11] Speech Microsoft: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx"> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                              </li>
                              <li>[12] Joint Filters: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx"> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                                </li>
                                <li> [13] Double Exponential Microsoft: Available at: <a href=" https://msdn.microsoft.com/en-us/library/jj127860.aspx "> https://msdn.microsoft.com/en-us/library/jj127860.aspx </a>
                                  </li>
                                  <li> [14] Command Pattern: Available at: <a href="https://sourcemaking.com/design_patterns/command"> https://sourcemaking.com/design_patterns/command</a></li>
                                  <li>
                                    [15] Gamma, Erich, et al. Design Patterns: Elements of Reusable Object-Oriented Software. Pearson Education, 2015.
                                  </li>
                                <li> [16] Functional Testing: Available at: <a href="https://www.guru99.com/functional-testing.html"> https://www.guru99.com/functional-testing.html</a> </li>
                                <li> [17] NSubstitute: Available at: <a href="http://nsubstitute.github.io/help.html"> http://nsubstitute.github.io/help.html</a> </li>
                                <li> [18] Unity Tests Documentation: Available at: <a href="https://docs.unity3d.com/Manual/testing-editortestsrunner.html"> https://docs.unity3d.com/Manual/testing-editortestsrunner.html</a> </li>
          </ul>


          <!-- <h3 class="mb-2">Contact Details</h>
          <h3 class="mb-0"> Location </h3>
          <p>Gower Street
          <br>London, WC1E 6HH</p>

          <h3 class="mb-0">The Team</h2>
          <p>Rares Dolga 
             | Fazaan Hassan |
              Calvan Black</p> -->
         <center> <p style = "margin-top: -10px">&copy; All rights reserved</p></center> 


        </div>
      </section>

    </div>


    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/resume.min.js"></script>


  </body>

</html>
